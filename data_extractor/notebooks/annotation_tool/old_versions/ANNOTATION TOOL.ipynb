{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANNOTATION TOOL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please change the settings to the needed ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_path = \"C:\\\\PythonScripting\\\\test\\\\NLP_ANNOTATION_TOOL\"\n",
    "output_path = \"C:\\\\PythonScripting\\\\test\\\\NLP_ANNOTATION_TOOL\\\\output\"\n",
    "annotator = \"David\"\n",
    "all_kpi = [0, 1, 2]\n",
    "# kpi_of_interest = all_kpi\n",
    "kpi_of_interest = [1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All KPI's are 0, 1, 2.\n",
      "KPI's of interest are 1, 2.\n"
     ]
    }
   ],
   "source": [
    "print(f\"All KPI's are \" + \", \".join([str(x) for x in all_kpi]) + \".\")\n",
    "print(f\"KPI's of interest are \" + \", \".join([str(x) for x in kpi_of_interest]) + \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preloading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading packages and the existing annotations (just execute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "df_annotations = pd.read_excel(annotations_path + \"\\\\annotations.xlsx\")\n",
    "df_out = df_annotations.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Annotations overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following gives an overview of the existing predictions and which file needs a further investigation (just execute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 output files in the output folder:\n",
      "test2_predictions_kpi.csv\n",
      "test_predictions_kpi.csv\n",
      "\n",
      " Next file considered: test2_predictions_kpi.csv\n",
      "For pdf with name \"test2.pdf\" in file test2_predictions_kpi.csv we have already annotations for the kpi's 1, 2.\n",
      "DONE: All kpi's of interest have been annotated for this file.\n",
      "\n",
      " Next file considered: test_predictions_kpi.csv\n",
      "For pdf with name \"test.pdf\" in file test_predictions_kpi.csv we have already annotations for the kpi's 1, 2.\n",
      "DONE: All kpi's of interest have been annotated for this file.\n"
     ]
    }
   ],
   "source": [
    "outputs = glob.glob(output_path + \"\\\\*\")\n",
    "outputs = [x.rsplit(\"\\\\\", 1)[1] for x in outputs]\n",
    "if len(outputs) > 1:\n",
    "    print(f\"There are {len(outputs)} output files in the output folder:\" + \"\\n\" + \"\\n\".join([str(x) for x in outputs]))\n",
    "elif len(outputs) == 1:\n",
    "    print(f\"There is one output file in the output folder.\")\n",
    "else:\n",
    "    print(f\"There are no new files in the output folder.\")\n",
    "\n",
    "\n",
    "for output in outputs:\n",
    "    print(\"\\n Next file considered: \" + output)\n",
    "    df_output = pd.read_csv(output_path + \"\\\\\" + output)\n",
    "    pdf_name = df_output[\"pdf_name\"].values[0]\n",
    "    df_annotations_temp = df_annotations[df_annotations[\"source_file\"] == pdf_name]\n",
    "    kpis_contained = [x for x in df_annotations_temp[\"kpi_id\"].values if x in kpi_of_interest]\n",
    "    if len(kpis_contained) > 1:\n",
    "        print(\n",
    "            'For pdf with name \"'\n",
    "            + pdf_name\n",
    "            + '\" in file '\n",
    "            + output\n",
    "            + \" we have already annotations for the kpi's \"\n",
    "            + \", \".join([str(x) for x in kpis_contained])\n",
    "            + \".\"\n",
    "        )\n",
    "    elif len(kpis_contained) == 1:\n",
    "        print(\n",
    "            'For pdf with name \"'\n",
    "            + pdf_name\n",
    "            + \"\\ in file \"\n",
    "            + output\n",
    "            + \" we have already an annotation for the kpi \"\n",
    "            + \",\".join([str(x) for x in kpis_contained])\n",
    "            + \".\"\n",
    "        )\n",
    "    else:\n",
    "        print('For pdf with name \"' + pdf_name + \"\\\" we have no annotations yet for the kpi's under investigation.\")\n",
    "    if kpis_contained == kpi_of_interest:\n",
    "        print(\"DONE: All kpi's of interest have been annotated for this file.\")\n",
    "    else:\n",
    "        print(\"TODO: There are open annotations for that file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Check new annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please set the file you want to investigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"test2_predictions_kpi.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of open tasks (just execute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no open kpi's.\n"
     ]
    }
   ],
   "source": [
    "df_output = pd.read_csv(output_path + \"\\\\\" + output_file)\n",
    "pdf_name = df_output[\"pdf_name\"].values[0]\n",
    "df_annotations_temp = df_annotations[df_annotations[\"source_file\"] == pdf_name]\n",
    "kpis_contained = [x for x in df_annotations_temp[\"kpi_id\"].values if x in kpi_of_interest]\n",
    "open_kpis = [x for x in kpi_of_interest if x not in kpis_contained]\n",
    "if len(open_kpis) > 1:\n",
    "    print(\"The open kpi's are \" + \", \".join([str(x) for x in open_kpis]) + \".\")\n",
    "elif len(open_kpis) == 1:\n",
    "    print(\"The open kpi is \" + \", \".join([str(x) for x in open_kpis]) + \".\")\n",
    "else:\n",
    "    print(\"There are no open kpi's.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Detailed investigation\n",
    "Please set the kpi you want to investigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpi_to_investigate = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the outcome of the machine (just execute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>pdf_name</th>\n",
       "      <th>kpi</th>\n",
       "      <th>kpi_id</th>\n",
       "      <th>answer</th>\n",
       "      <th>page</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>source</th>\n",
       "      <th>score</th>\n",
       "      <th>no_ans_score</th>\n",
       "      <th>no_answer_score_plus_boost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>test2.pdf</td>\n",
       "      <td>KPI 2</td>\n",
       "      <td>2</td>\n",
       "      <td>test2</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[test]</td>\n",
       "      <td>Text</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>test2.pdf</td>\n",
       "      <td>KPI 2</td>\n",
       "      <td>2</td>\n",
       "      <td>test2</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[test]</td>\n",
       "      <td>Text</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>test2.pdf</td>\n",
       "      <td>KPI 2</td>\n",
       "      <td>2</td>\n",
       "      <td>test2</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[test]</td>\n",
       "      <td>Text</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>test2.pdf</td>\n",
       "      <td>KPI 2</td>\n",
       "      <td>2</td>\n",
       "      <td>test2</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[test]</td>\n",
       "      <td>Text</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0   pdf_name    kpi  kpi_id answer page paragraph source  score  \\\n",
       "8            9  test2.pdf  KPI 2       2  test2  [0]    [test]   Text      0   \n",
       "9           10  test2.pdf  KPI 2       2  test2  [0]    [test]   Text      1   \n",
       "10          11  test2.pdf  KPI 2       2  test2  [0]    [test]   Text      2   \n",
       "11          12  test2.pdf  KPI 2       2  test2  [0]    [test]   Text      3   \n",
       "\n",
       "    no_ans_score  no_answer_score_plus_boost  \n",
       "8              0                           0  \n",
       "9              1                           1  \n",
       "10             2                           2  \n",
       "11             3                           3  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check specific KPIs\n",
    "df_output = pd.read_csv(output_path + \"\\\\\" + output_file)\n",
    "df_output_check = df_output[[x == kpi_to_investigate for x in df_output[\"kpi_id\"]]]\n",
    "df_output_check.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Set answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the id where one can find the correct paragraph and/or the answer. In case an optimal paragraph and/or answer does not exist, please specify it in the variables \"correct_*\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_correct_paragraph = 9\n",
    "id_correct_answer = 10\n",
    "\n",
    "# Only if paragraph is not contained\n",
    "correct_paragraph = \"David wears a red shirt.\"\n",
    "correct_paragraph_page = 24\n",
    "correct_paragraph_source = \"TEXT\"  # Either \"TEXT\" or \"TABLE\"\n",
    "\n",
    "# Only if answer is not contained\n",
    "correct_answer = \"red\"\n",
    "\n",
    "company = \"DUMMY\"\n",
    "year = 2022\n",
    "sector = \"OG\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Generate annotation outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having set the correct annotations we can generate a new entry for the annotations file (just execute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>company</th>\n",
       "      <th>source_file</th>\n",
       "      <th>source_page</th>\n",
       "      <th>kpi_id</th>\n",
       "      <th>year</th>\n",
       "      <th>answer</th>\n",
       "      <th>data_type</th>\n",
       "      <th>relevant_paragraphs</th>\n",
       "      <th>annotator</th>\n",
       "      <th>sector</th>\n",
       "      <th>issue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>TEST_INC</td>\n",
       "      <td>test.pdf</td>\n",
       "      <td>[0]</td>\n",
       "      <td>2</td>\n",
       "      <td>2020</td>\n",
       "      <td>test</td>\n",
       "      <td>TEXT</td>\n",
       "      <td>[test]</td>\n",
       "      <td>Tester</td>\n",
       "      <td>OG</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>TEST_INC_2</td>\n",
       "      <td>test2.pdf</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0</td>\n",
       "      <td>2020</td>\n",
       "      <td>test2</td>\n",
       "      <td>TEXT</td>\n",
       "      <td>[test]</td>\n",
       "      <td>Tester</td>\n",
       "      <td>OG</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>DUMMY</td>\n",
       "      <td>test2.pdf</td>\n",
       "      <td>[0]</td>\n",
       "      <td>1</td>\n",
       "      <td>2022</td>\n",
       "      <td>te</td>\n",
       "      <td>Text</td>\n",
       "      <td>[test]</td>\n",
       "      <td>David</td>\n",
       "      <td>OG</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>DUMMY</td>\n",
       "      <td>test2.pdf</td>\n",
       "      <td>[0]</td>\n",
       "      <td>2</td>\n",
       "      <td>2022</td>\n",
       "      <td>test2</td>\n",
       "      <td>Text</td>\n",
       "      <td>[test]</td>\n",
       "      <td>David</td>\n",
       "      <td>OG</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   number     company source_file source_page  kpi_id  year answer data_type  \\\n",
       "2       2    TEST_INC    test.pdf         [0]       2  2020   test      TEXT   \n",
       "3       3  TEST_INC_2   test2.pdf         [0]       0  2020  test2      TEXT   \n",
       "4       4       DUMMY   test2.pdf         [0]       1  2022     te      Text   \n",
       "5       5       DUMMY   test2.pdf         [0]       2  2022  test2      Text   \n",
       "\n",
       "  relevant_paragraphs annotator sector issue  \n",
       "2              [test]    Tester     OG   NaN  \n",
       "3              [test]    Tester     OG   NaN  \n",
       "4              [test]     David     OG   NaN  \n",
       "5              [test]     David     OG        "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp = df_annotations.head(0)\n",
    "if id_correct_paragraph == -1:\n",
    "    paragraph = \"[\" + correct_paragraph + \"]\"\n",
    "    source_page = \"[\" + str(correct_paragraph_page) + \"]\"\n",
    "    source = correct_paragraph_source\n",
    "else:\n",
    "    paragraph = df_output_check.loc[id_correct_paragraph, \"paragraph\"]\n",
    "    source_page = df_output_check.loc[id_correct_paragraph, \"page\"]\n",
    "    source = df_output_check.loc[id_correct_paragraph, \"source\"]\n",
    "\n",
    "if id_correct_answer == -1:\n",
    "    answer = correct_answer\n",
    "else:\n",
    "    answer = df_output_check.loc[id_correct_paragraph, \"answer\"]\n",
    "\n",
    "\n",
    "new_data = [\n",
    "    np.max(df_out[\"number\"].values) + 1,\n",
    "    company,\n",
    "    df_output_check[\"pdf_name\"].values[0],\n",
    "    source_page,\n",
    "    kpi_to_investigate,\n",
    "    year,\n",
    "    answer,\n",
    "    source,\n",
    "    paragraph,\n",
    "    annotator,\n",
    "    sector,\n",
    "    \"\",\n",
    "]\n",
    "df_series = pd.Series(new_data, index=df_temp.columns)\n",
    "df_temp = df_temp.append(df_series, ignore_index=True)\n",
    "df_temp = df_temp.set_index([pd.Index([np.max(df_out.index) + 1])])\n",
    "df_out = df_out.append(df_temp)\n",
    "df_out.tail(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if there are still open kpi's. If yes start again at point 4.1. (just execute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no open kpi's.\n"
     ]
    }
   ],
   "source": [
    "df_out_temp = df_out[df_out[\"source_file\"] == pdf_name]\n",
    "kpis_contained = [x for x in df_out_temp[\"kpi_id\"].values if x in kpi_of_interest]\n",
    "open_kpis = [x for x in kpi_of_interest if x not in kpis_contained]\n",
    "if len(open_kpis) > 1:\n",
    "    print(\"The open kpi's are \" + \", \".join([str(x) for x in open_kpis]) + \".\")\n",
    "elif len(open_kpis) == 1:\n",
    "    print(\"The open kpi is \" + \", \".join([str(x) for x in open_kpis]) + \".\")\n",
    "else:\n",
    "    print(\"There are no open kpi's.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Export outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.to_excel(annotations_path + \"\\\\annotations.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: After having exported the new annotations please start with the jupyter notebook from the beginning if you want to check the annotations of another file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In output file year, company and sector are missing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
