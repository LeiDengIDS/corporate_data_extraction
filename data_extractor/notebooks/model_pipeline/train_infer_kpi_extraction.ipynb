{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will curate the data and output them as SQuAD-like format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kpi_inference_data_pipeline import TextKPIInferenceCurator\n",
    "from kpi_inference_data_pipeline import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:1048: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-14 22:39:24,370 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Energetický a průmyslový holding, a.s. (EPH) Sustainablity Report 2018.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:24,372 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Energetický a průmyslový holding, a.s. (EPH) Sustainablity Report 2018.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:24,373 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Energetický a průmyslový holding, a.s. (EPH) Sustainablity Report 2018.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:24,375 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Energetický a průmyslový holding, a.s. (EPH) Sustainablity Report 2018.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:24,376 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Energetický a průmyslový holding, a.s. (EPH) Sustainablity Report 2018.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:24,377 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Energetický a průmyslový holding, a.s. (EPH) Sustainablity Report 2018.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:24,378 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Energetický a průmyslový holding, a.s. (EPH) Sustainablity Report 2018.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:24,402 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — vopak_annual_report_2017.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:24,403 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — vopak_annual_report_2017.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:24,404 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — LUKOIL_SUSTAINABILITY_REPORT_2018.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:24,406 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — LUKOIL_SUSTAINABILITY_REPORT_2018.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:24,407 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — LUKOIL_SUSTAINABILITY_REPORT_2018.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:24,409 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — ar_2019_e.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:24,410 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — ar_2019_e.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:24,412 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — ar_2019_e.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:24,413 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — ar_2019_e.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:24,415 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — ar_2019_e.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:24,416 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — ar_2019_e.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:24,418 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — ar_2019_e.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:24,420 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — ar_2019_e.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:24,422 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:326 — sr_2019_e.json has empty list of paragraphs at page 1.                     Will use relevant text as annotated\n",
      "2020-10-14 22:39:24,423 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:326 — sr_2019_e.json has empty list of paragraphs at page 1.                     Will use relevant text as annotated\n",
      "2020-10-14 22:39:25,151 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Wintershall Dea annual report 2019.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:25,153 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Wintershall Dea annual report 2019.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:25,154 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Wintershall Dea annual report 2019.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:25,156 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Wintershall Dea annual report 2019.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:25,157 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Wintershall-Dea_Sustainability_Report_2019.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:25,159 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Wintershall-Dea_Sustainability_Report_2019.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:25,160 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:326 — annual_report_2019_eng.json has empty list of paragraphs at page 1.                     Will use relevant text as annotated\n",
      "2020-10-14 22:39:25,161 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:326 — annual_report_2019_eng.json has empty list of paragraphs at page 1.                     Will use relevant text as annotated\n",
      "2020-10-14 22:39:25,609 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:326 — annual 2017.json has empty list of paragraphs at page 1.                     Will use relevant text as annotated\n",
      "2020-10-14 22:39:25,610 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:326 — annual 2017.json has empty list of paragraphs at page 1.                     Will use relevant text as annotated\n",
      "2020-10-14 22:39:25,881 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:326 — annual 2015.json has empty list of paragraphs at page 1.                     Will use relevant text as annotated\n",
      "2020-10-14 22:39:25,882 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:326 — annual 2015.json has empty list of paragraphs at page 1.                     Will use relevant text as annotated\n",
      "2020-10-14 22:39:25,899 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Rosneft_CSR18_EN_Book sustainabilitz 2018.pdf json file has not been extracted. Will use relevant text as annotated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-14 22:39:25,900 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Rosneft_CSR18_EN_Book sustainabilitz 2018.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:25,901 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Rosneft_CSR18_EN_Book sustainabilitz 2018.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:25,903 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Rosneft_CSR18_EN_Book sustainabilitz 2018.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:25,904 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Rosneft_CSR18_EN_Book sustainabilitz 2018.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:25,906 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Rosneft_CSR18_EN_Book sustainabilitz 2018.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:25,908 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:326 — RN_SR2018_eng_web_1 sustainability 2017.json has empty list of paragraphs at page 1.                     Will use relevant text as annotated\n",
      "2020-10-14 22:39:25,909 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:326 — RN_SR2018_eng_web_1 sustainability 2017.json has empty list of paragraphs at page 1.                     Will use relevant text as annotated\n",
      "2020-10-14 22:39:26,636 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:326 — Aker-BP-Annual-report-2018.json has empty list of paragraphs at page 1.                     Will use relevant text as annotated\n",
      "2020-10-14 22:39:26,638 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:326 — Aker-BP-Annual-report-2018.json has empty list of paragraphs at page 1.                     Will use relevant text as annotated\n",
      "2020-10-14 22:39:27,101 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:326 — AKERBP-Annual-Report-2017.json has empty list of paragraphs at page 1.                     Will use relevant text as annotated\n",
      "2020-10-14 22:39:27,103 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:326 — AKERBP-Annual-Report-2017.json has empty list of paragraphs at page 1.                     Will use relevant text as annotated\n",
      "2020-10-14 22:39:27,173 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:326 — AKERBP-Annual-Report-2016.json has empty list of paragraphs at page 1.                     Will use relevant text as annotated\n",
      "2020-10-14 22:39:27,175 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:326 — AKERBP-Annual-Report-2016.json has empty list of paragraphs at page 1.                     Will use relevant text as annotated\n",
      "2020-10-14 22:39:29,070 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:326 — Aker-BP-Sustainability-Report-2018-1.json has empty list of paragraphs at page 1.                     Will use relevant text as annotated\n",
      "2020-10-14 22:39:29,072 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:326 — Aker-BP-Sustainability-Report-2018-1.json has empty list of paragraphs at page 1.                     Will use relevant text as annotated\n",
      "2020-10-14 22:39:30,235 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:326 — Aker-BP-Sustainability-report-2017.json has empty list of paragraphs at page 1.                     Will use relevant text as annotated\n",
      "2020-10-14 22:39:30,237 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:326 — Aker-BP-Sustainability-report-2017.json has empty list of paragraphs at page 1.                     Will use relevant text as annotated\n",
      "2020-10-14 22:39:30,319 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Ameren_2019_Annual_Report.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:30,320 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Ameren_2019_Annual_Report.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:30,321 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Ameren_2019_Annual_Report.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:30,323 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Ameren_2019_Annual_Report.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:30,324 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Ameren_2019_Annual_Report.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:30,326 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Ameren_2019_Annual_Report.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:30,327 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Ameren_2019_Annual_Report.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:30,329 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Ameren_2019_Annual_Report.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:30,331 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Ameren_2019_Annual_Report.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:30,332 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Ameren_2019_Annual_Report.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:30,334 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Ameren_2019_Annual_Report.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:30,421 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:326 — AGL Energy Ltd Annual Report 2019.json has empty list of paragraphs at page 1.                     Will use relevant text as annotated\n",
      "2020-10-14 22:39:30,423 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:326 — AGL Energy Ltd Annual Report 2019.json has empty list of paragraphs at page 1.                     Will use relevant text as annotated\n",
      "2020-10-14 22:39:30,656 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:326 — AGL Energy Ltd FY19 Carbon Scenario Analysis.json has empty list of paragraphs at page 1.                     Will use relevant text as annotated\n",
      "2020-10-14 22:39:30,658 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:326 — AGL Energy Ltd FY19 Carbon Scenario Analysis.json has empty list of paragraphs at page 1.                     Will use relevant text as annotated\n",
      "2020-10-14 22:39:30,763 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Adani Groupl Adani Enterprises Sustainability Report 2019.pdf json file has not been extracted. Will use relevant text as annotated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-14 22:39:30,765 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Adani Groupl Adani Enterprises Sustainability Report 2019.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:30,767 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Adani Groupl Adani Enterprises Sustainability Report 2019.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:30,768 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Adani Groupl Adani Enterprises Sustainability Report 2019.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:30,770 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Adani Groupl Adani Enterprises Sustainability Report 2019.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:32,210 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:326 — NYSE_TOT_2016 annual.json has empty list of paragraphs at page 1.                     Will use relevant text as annotated\n",
      "2020-10-14 22:39:32,211 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:326 — NYSE_TOT_2016 annual.json has empty list of paragraphs at page 1.                     Will use relevant text as annotated\n",
      "2020-10-14 22:39:33,599 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:326 — NYSE_TOT_2017 annual.json has empty list of paragraphs at page 1.                     Will use relevant text as annotated\n",
      "2020-10-14 22:39:33,600 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:326 — NYSE_TOT_2017 annual.json has empty list of paragraphs at page 1.                     Will use relevant text as annotated\n",
      "2020-10-14 22:39:34,928 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:326 — NYSE_TOT_2018 annual.json has empty list of paragraphs at page 1.                     Will use relevant text as annotated\n",
      "2020-10-14 22:39:34,930 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:326 — NYSE_TOT_2018 annual.json has empty list of paragraphs at page 1.                     Will use relevant text as annotated\n",
      "2020-10-14 22:39:35,519 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:326 — annual 2017.json has empty list of paragraphs at page 1.                     Will use relevant text as annotated\n",
      "2020-10-14 22:39:35,736 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:326 — gb_2019-en_interaktiv.json has empty list of paragraphs at page 1.                     Will use relevant text as annotated\n",
      "2020-10-14 22:39:35,879 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:326 — 28022019-Repsol-Annual-Financial-Report-2018_tcm14-147383.json has empty list of paragraphs at page 1.                     Will use relevant text as annotated\n",
      "2020-10-14 22:39:35,940 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:326 — sustainability-report-repsol-2016-eng-april-baja_tcm14-63403.json has empty list of paragraphs at page 1.                     Will use relevant text as annotated\n",
      "2020-10-14 22:39:35,941 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — AES Corporation 2018_SustainabilityReport_vFinal.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:35,942 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — AES Corporation 2018_SustainabilityReport_vFinal.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:36,204 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:326 — Enel annual-report-2018.json has empty list of paragraphs at page 2.                     Will use relevant text as annotated\n",
      "2020-10-14 22:39:36,560 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Enel annual-report-2017.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:36,561 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Enel annual-report-2017.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:36,562 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Enel SA sustainability-report-2017.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:36,563 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Enel SA sustainability-report-2017.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:36,565 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Enel SA sustainability-report-2017.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:36,566 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Enel SA sustainability-report-2017.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:36,568 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Enel SA sustainability-report-2017.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:36,571 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Enel SA sustainability-report-2017.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:36,573 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Enel SA sustainability-report-2017.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:36,574 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Endesa SA Annual Report 2017.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:36,575 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Endesa SA Annual Report 2017.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:37,278 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:326 — RWE Investor Presentation.json has empty list of paragraphs at page 1.                     Will use relevant text as annotated\n",
      "2020-10-14 22:39:37,282 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:326 — RWE-csr_overall-report-2019.json has empty list of paragraphs at page 1.                     Will use relevant text as annotated\n",
      "2020-10-14 22:39:37,481 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:326 — RWE-factbook_report-2018.json has empty list of paragraphs at page 1.                     Will use relevant text as annotated\n",
      "2020-10-14 22:39:37,482 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:326 — RWE-factbook_report-2018.json has empty list of paragraphs at page 1.                     Will use relevant text as annotated\n",
      "2020-10-14 22:39:37,489 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:326 — Vietnam Electricity Corporation (EVN)_AnnualReport2018(1).json has empty list of paragraphs at page 1.                     Will use relevant text as annotated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-14 22:39:37,641 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:326 — Sustainability_Report_2017_Eng_small[1].json has empty list of paragraphs at page 1.                     Will use relevant text as annotated\n",
      "2020-10-14 22:39:38,086 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:326 — dload_sustainabilityreport2019_EN.json has empty list of paragraphs at page 1.                     Will use relevant text as annotated\n",
      "2020-10-14 22:39:38,171 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:326 — odfjell-se-annual-report-2018.json has empty list of paragraphs at page 1.                     Will use relevant text as annotated\n",
      "2020-10-14 22:39:38,544 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:326 — Sustainability Report 2016_EN.json has empty list of paragraphs at page 1.                     Will use relevant text as annotated\n",
      "2020-10-14 22:39:38,600 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:326 — Sustainability Report 2017_EN.json has empty list of paragraphs at page 1.                     Will use relevant text as annotated\n",
      "2020-10-14 22:39:39,246 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — EDF-ddr-2017-accessible-version-en.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,247 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — EDF-ddr-2017-accessible-version-en.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,248 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — EDF-ddr-2017-accessible-version-en.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,250 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — EDF-ddr-2017-accessible-version-en.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,252 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — EDF-ddr-2017-accessible-version-en.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,253 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — EDF-ddr-2017-accessible-version-en.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,254 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — EDF-ddr-2017-accessible-version-en.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,256 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — EDF-ddr-2017-accessible-version-en.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,257 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — EDF-ddr-2017-accessible-version-en.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,259 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — EDF-ddr-2017-accessible-version-en.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,260 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — EDF-ddr-2017-accessible-version-en.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,262 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — EDF-ddr-2017-accessible-version-en.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,263 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — EDF-ddr-2017-accessible-version-en.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,264 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — EDF-ddr-2017-accessible-version-en.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,266 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — EDF-ddr-2018-en.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,267 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — EDF-ddr-2018-en.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,268 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — EDF-ddr-2018-en.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,269 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — EDF-ddr-2018-en.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,270 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — EDF-ddr-2018-en.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,272 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — EDF-ddr-2018-en.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,273 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — EDF-ddr-2018-en.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,275 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — EDF-ddr-2018-en.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,276 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — EDF-ddr-2018-en.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,277 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — EDF-ddr-2018-en.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,279 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — EDF-ddr-2018-en.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,280 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — EDF-urd-annual-financial-report-2019-en-2.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,281 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — EDF-urd-annual-financial-report-2019-en-2.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,283 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — EDF-urd-annual-financial-report-2019-en-2.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,284 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — EDF-urd-annual-financial-report-2019-en-2.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,286 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — EDF-urd-annual-financial-report-2019-en-2.pdf json file has not been extracted. Will use relevant text as annotated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-14 22:39:39,287 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — EDF-urd-annual-financial-report-2019-en-2.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,289 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — EDF-urd-annual-financial-report-2019-en-2.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,290 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — EDF-urd-annual-financial-report-2019-en-2.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,292 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — EDF-urd-annual-financial-report-2019-en-2.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,293 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — EDF-urd-annual-financial-report-2019-en-2.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,294 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — EDF-urd-annual-financial-report-2019-en-2.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,295 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — EDF-urd-annual-financial-report-2019-en-2.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,296 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — EDF-urd-annual-financial-report-2019-en-2.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,298 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — EDF-urd-annual-financial-report-2019-en-2.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,299 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — EDF-urd-annual-financial-report-2019-en-2.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,300 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — EDF-urd-annual-financial-report-2019-en-2.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,301 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — EDF-urd-annual-financial-report-2019-en-2.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,302 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — EDF-urd-annual-financial-report-2019-en-2.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,303 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Southern Company_2017_Corporate_Responsibility_Report.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,305 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Southern Company_2017_Corporate_Responsibility_Report.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,307 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Southern Company_2017_Corporate_Responsibility_Report.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,308 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Southern Company_2017_Corporate_Responsibility_Report.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,310 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Southern Company_2017_Corporate_Responsibility_Report.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,312 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Southern Company_2017_Corporate_Responsibility_Report.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,313 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Southern Company_2017_Corporate_Responsibility_Report.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,314 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Southern Company_2017_Corporate_Responsibility_Report.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,316 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Southern Company_2017_Corporate_Responsibility_Report.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,360 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — mol_plc_s_parent_company_annual_report_2017_en.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,361 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — mol_plc_s_parent_company_annual_report_2017_en.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,362 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — mol_plc_s_parent_company_annual_report_2017_en.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,363 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — mol_plc_s_parent_company_annual_report_2017_en.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,364 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — ng-annual-report-and-accounts-2018-19.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,366 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — ng-annual-report-and-accounts-2018-19.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,368 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — ng-annual-report-and-accounts-2018-19.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,369 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — ng-annual-report-and-accounts-2018-19.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,371 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — ng-annual-report-and-accounts-2018-19.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,372 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — ng-annual-report-and-accounts-2018-19.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,374 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — ng-annual-report-and-accounts-2018-19.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:39,745 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — national-grid-plc-annual-report-and-accounts-2015-17.pdf json file has not been extracted. Will use relevant text as annotated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-14 22:39:39,747 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — national-grid-plc-annual-report-and-accounts-2015-17.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:40,345 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:326 — PAO OGK-2_AR 2018.json has empty list of paragraphs at page 1.                     Will use relevant text as annotated\n",
      "2020-10-14 22:39:40,348 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — ANNUAL REPORT 2017.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:40,349 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — ANNUAL REPORT 2017.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:40,355 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:326 — SAIPEM SUSTAINABILITY 2016.json has empty list of paragraphs at page 1.                     Will use relevant text as annotated\n",
      "2020-10-14 22:39:40,407 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — TGS-AR2019-FINAL-WEB-PAGES-reduced annual.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:40,408 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — TGS-AR2019-FINAL-WEB-PAGES-reduced annual.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:40,409 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — TGS-AR2019-FINAL-WEB-PAGES-reduced annual.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:40,411 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — TGS-AR2019-FINAL-WEB-PAGES-reduced annual.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:45,642 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — 2_LOTOS_Group Directors Report 2019.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:45,644 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — 2_LOTOS_Group Directors Report 2019.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:45,645 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — 2_LOTOS_Group Directors Report 2019.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:45,647 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — 2_LOTOS_Group Directors Report 2019.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:45,648 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — 2_LOTOS_Group Directors Report 2019.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:45,650 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — 2_LOTOS_Group Directors Report 2019.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:45,651 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — 2_LOTOS_Group Directors Report 2019.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:45,653 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — 2_LOTOS_Group Directors Report 2019.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:45,654 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — 2_LOTOS_Group Directors Report 2019.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:45,655 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — 2_LOTOS_Group Directors Report 2019.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:45,656 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — 2_LOTOS_Group Directors Report 2019.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:46,092 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — 2017_SustainabilityReport_2_9_Web.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:46,093 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — 2017_SustainabilityReport_2_9_Web.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:48,254 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — PJSC Tatneft annual report 2016.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:48,256 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — PJSC Tatneft annual report 2016.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:48,258 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — PJSC Tatneft annual report 2016.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:48,259 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — PJSC Tatneft annual report 2016.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:48,261 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — PJSC Tatneft annual report 2016.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:48,263 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — PJSC Tatneft annual report 2016.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:48,265 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — PJSC Tatneft annual report 2016.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:48,266 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — PJSC Tatneft annual report 2016.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:48,268 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — PJSC Tatneft annual report 2016.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:48,270 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — PJSC Tatneft annual report 2016.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:48,670 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — University of Plymouth Sustainability_Report_2018.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:48,672 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — University of Plymouth Sustainability_Report_2018.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:48,673 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — University of Plymouth Sustainability_Report_2018.pdf json file has not been extracted. Will use relevant text as annotated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-14 22:39:48,674 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — University of Plymouth Sustainability_Report_2018.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:48,675 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — University of Plymouth Sustainability_Report_2018.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:48,677 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — University of Plymouth Sustainability_Report_2018.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:50,293 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Elektrik Üretim A.Ş. Genel Müdürlüğü (EÜAŞ) Annual Report 2018.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:50,295 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Elektrik Üretim A.Ş. Genel Müdürlüğü (EÜAŞ) Annual Report 2018.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:50,296 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Elektrik Üretim A.Ş. Genel Müdürlüğü (EÜAŞ) Annual Report 2018.pdf json file has not been extracted. Will use relevant text as annotated.\n",
      "2020-10-14 22:39:50,298 — kpi_inference_data_pipeline.components.text_kpi_inference_curator — INFO —return_full_paragraph:315 — Elektrik Üretim A.Ş. Genel Müdürlüğü (EÜAŞ) Annual Report 2018.pdf json file has not been extracted. Will use relevant text as annotated.\n"
     ]
    }
   ],
   "source": [
    "tkpi = TextKPIInferenceCurator(**config.TextKPIInferenceCurator_kwargs)\n",
    "train_squad, val_squad = tkpi.curate(**config.CurateConfig().__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have the data in SQuAD format, which is ready for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__.py\r\n",
      "aggregated_annotation.csv\r\n",
      "annotations\r\n",
      "dev_split_esg_kpi_train_13-10-2020.json\r\n",
      "esg_kpi_13-10-2020.csv\r\n",
      "esg_kpi_14-10-2020.csv\r\n",
      "esg_kpi_train_13-10-2020.json\r\n",
      "esg_kpi_train_14-10-2020.json\r\n",
      "negative_summary.csv\r\n",
      "positive_summary.csv\r\n",
      "text_3434.csv\r\n",
      "text_extraction\r\n",
      "train_split_esg_kpi_train_13-10-2020.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls $config.DATA_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KPI extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This following shows how to train a kpi extraction model and make inference using that. <br> <br>\n",
    "KPI extraction model is a Question-Answering (QA) system. A public dataset for the Question-Answering task is called SQuAD. This notebook assumes that the ESG data is already curated in a SQuAD-like format. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our pipeline includes components that are provided by the FARM library. FARM is a framework which facilitates transfer learning tasks for BERT based models. Documentation for FARM is available here: https://farm.deepset.ai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## General imports\n",
    "import pprint\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Set the configurable parameters. \n",
    "Before start training, parameters for each component of the training pipeline must be set. For this we create `config` objects which hold these parameters. Default values have already been set but they can be easily changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from model_pipeline import QAFARMTrainer\n",
    "from model_pipeline import (\n",
    "    QAFileConfig,\n",
    "    QATokenizerConfig,\n",
    "    QAProcessorConfig,\n",
    "    QAModelConfig,\n",
    "    QATrainingConfig,\n",
    "    QAMLFlowConfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_config = QAFileConfig()  # Settings data files and checkpoints parameters\n",
    "processor_config = QAProcessorConfig()  # Settings for the processor component\n",
    "tokenizer_config = QATokenizerConfig()  # Settings for the tokenizer\n",
    "model_config = QAModelConfig()  # Settings for the model\n",
    "train_config = QATrainingConfig()  # Settings for training\n",
    "mlflow_config = QAMLFlowConfig()  # Settings for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters can be changed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_config.experiment_name = \"demo_training\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we advise that you manually update the parameters in the corresponding config file:\n",
    "\n",
    "`model_pipeline/model_pipeline/config_qa_farm_trainer.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should change the training data file to the one we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/model_pipeline/model_pipeline/data/esg_kpi_train_13-10-2020.json'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"/model_pipeline/model_pipeline/data/\"\n",
    "curated_data_path = os.path.join(data_path, \"esg_kpi_train_15-10-2020.json\")\n",
    "file_config.update_paths(data_path, curated_data_path)\n",
    "file_config.perform_splitting = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the value for some parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment_name: \n",
      " demo_training \n",
      "\n",
      "Data directory: \n",
      " /model_pipeline/model_pipeline/data \n",
      "\n",
      "Curated dataset path: \n",
      " /model_pipeline/model_pipeline/data/esg_kpi_train_14-10-2020.json \n",
      "\n",
      "Split train/validation ratio: \n",
      "0.2 \n",
      "\n",
      "Training dataset path: \n",
      " /model_pipeline/model_pipeline/data/train_split_esg_kpi_train_13-10-2020.json \n",
      "\n",
      "Validation dataset path: \n",
      " /model_pipeline/model_pipeline/data/dev_split_esg_kpi_train_13-10-2020.json \n",
      "\n",
      "Directory where trained model is saved: \n",
      " /model_pipeline/model_pipeline/saved_models/test_farm \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Experiment_name: \\n {file_config.experiment_name} \\n\")\n",
    "print(f\"Data directory: \\n {file_config.data_dir} \\n\")\n",
    "print(f\"Curated dataset path: \\n {file_config.curated_data} \\n\")\n",
    "print(f\"Split train/validation ratio: \\n{file_config.dev_split} \\n\")\n",
    "print(f\"Training dataset path: \\n {file_config.train_filename} \\n\")\n",
    "print(f\"Validation dataset path: \\n {file_config.dev_filename} \\n\")\n",
    "print(f\"Directory where trained model is saved: \\n {file_config.saved_models_dir} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of tokens per example: 384 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Max number of tokens per example: {processor_config.max_seq_len} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: True \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Use GPU: {train_config.use_cuda} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning_rate: 2e-05 \n",
      "\n",
      "Number of epochs for fine tuning: 4 \n",
      "\n",
      "Batch size: 4 \n",
      "\n",
      "Perform Cross validation: False \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Learning_rate: {train_config.learning_rate} \\n\")\n",
    "print(f\"Number of epochs for fine tuning: {train_config.n_epochs} \\n\")\n",
    "print(f\"Batch size: {train_config.batch_size} \\n\")\n",
    "print(f\"Perform Cross validation: {train_config.run_cv} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training pipeline recieves the curated ESG dataset for KPI extraction. The necassary components includes the Tokenizer and Processor are loaded. These components will create features from the input text. Next, the model will be defined, the model is a bert-based model with extra dense layers for the question answering task. The weights of the model are initialized from the pretrained model on SQuAD dataset. In the Training phase the model will be fine-tuned on the ESG curated data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tune on curated ESG data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all the parameters are set, a `QAFARMTrainer` object can be instantiated by passing all the configuration objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "farm_trainer = QAFARMTrainer(\n",
    "    file_config=file_config,\n",
    "    tokenizer_config=tokenizer_config,\n",
    "    model_config=model_config,\n",
    "    processor_config=processor_config,\n",
    "    training_config=train_config,\n",
    "    mlflow_config=mlflow_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the method `run()` to start training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note_ For the first time, loading the model will take a little longer, for download the checkpoints. The model will be cached after that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/14/2020 22:43:34 - INFO - model_pipeline.qa_farm_trainer -   Loading the train from /model_pipeline/model_pipeline/data/train_split_esg_kpi_train_13-10-2020.json \n",
      " Loading validation data from /model_pipeline/model_pipeline/data/dev_split_esg_kpi_train_13-10-2020.json\n",
      "10/14/2020 22:43:34 - INFO - farm.utils -   device: cuda n_gpu: 1, distributed training: False, automatic mixed precision training: True\n",
      "10/14/2020 22:43:34 - INFO - farm.modeling.tokenization -   Loading tokenizer of type 'RobertaTokenizer'\n",
      "2020/10/14 22:43:36 WARNING mlflow.tracking.context: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.\n",
      "The git executable must be specified in one of the following ways:\n",
      "    - be included in your $PATH\n",
      "    - be set via $GIT_PYTHON_GIT_EXECUTABLE\n",
      "    - explicitly set via git.refresh()\n",
      "\n",
      "All git commands will error until this is rectified.\n",
      "\n",
      "This initial warning can be silenced or aggravated in the future by setting the\n",
      "$GIT_PYTHON_REFRESH environment variable. Use one of the following values:\n",
      "    - quiet|q|silence|s|none|n|0: for no warning or exception\n",
      "    - warn|w|warning|1: for a printed warning\n",
      "    - error|e|raise|r|2: for a raised exception\n",
      "\n",
      "Example:\n",
      "    export GIT_PYTHON_REFRESH=quiet\n",
      "\n",
      "10/14/2020 22:43:36 - INFO - farm.data_handler.data_silo -   \n",
      "Loading data into the data silo ... \n",
      "              ______\n",
      "               |o  |   !\n",
      "   __          |:`_|---'-.\n",
      "  |__|______.-/ _ \\-----.|       \n",
      " (o)(o)------'\\ _ /     ( )      \n",
      " \n",
      "10/14/2020 22:43:36 - INFO - farm.data_handler.data_silo -   Loading train set from: /model_pipeline/model_pipeline/data/train_split_esg_kpi_train_13-10-2020.json \n",
      "10/14/2020 22:43:36 - INFO - farm.data_handler.data_silo -   Got ya 15 parallel workers to convert 783 dictionaries to pytorch datasets (chunksize = 11)...\n",
      "10/14/2020 22:43:36 - INFO - farm.data_handler.data_silo -    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0 \n",
      "10/14/2020 22:43:36 - INFO - farm.data_handler.data_silo -   /w\\  /w\\  /w\\  /w\\  /w\\  /w\\  /w\\  /|\\  /w\\  /w\\  /w\\  /w\\  /w\\  /w\\  /|\\\n",
      "10/14/2020 22:43:36 - INFO - farm.data_handler.data_silo -   /'\\  / \\  /'\\  /'\\  / \\  / \\  /'\\  /'\\  /'\\  /'\\  /'\\  /'\\  / \\  /'\\  /'\\\n",
      "10/14/2020 22:43:36 - INFO - farm.data_handler.data_silo -                               \n",
      "Preprocessing Dataset /model_pipeline/model_pipeline/data/train_split_esg_kpi_train_13-10-2020.json:   0%|          | 0/783 [00:00<?, ? Dicts/s]10/14/2020 22:43:37 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
      "10/14/2020 22:43:37 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 3-0-0\n",
      "Clear Text: \n",
      " \tpassage_text: the power sector in india is composed of coal, gas, hydroelectric, renewable and nuclear energy. total installed capacity as of december 31, 2018 was 349 gw, of which 64% is thermal generation. renewable energy is adding capacity at a rapid pace and currently represents 21% of the total installed capacity. the remaining capacity is nuclear 2% and hydro 13%.\n",
      " \tquestion_text: What is the annual total production from coal?\n",
      " \tpassage_id: 0\n",
      " \tanswers: []\n",
      "Tokenized: \n",
      " \tpassage_start_t: 0\n",
      " \tpassage_tokens: ['the', 'Ġpower', 'Ġsector', 'Ġin', 'Ġind', 'ia', 'Ġis', 'Ġcomposed', 'Ġof', 'Ġcoal', ',', 'Ġgas', ',', 'Ġhydro', 'electric', ',', 'Ġrenewable', 'Ġand', 'Ġnuclear', 'Ġenergy', '.', 'Ġtotal', 'Ġinstalled', 'Ġcapacity', 'Ġas', 'Ġof', 'Ġde', 'cember', 'Ġ31', ',', 'Ġ2018', 'Ġwas', 'Ġ349', 'Ġg', 'w', ',', 'Ġof', 'Ġwhich', 'Ġ64', '%', 'Ġis', 'Ġthermal', 'Ġgeneration', '.', 'Ġrenewable', 'Ġenergy', 'Ġis', 'Ġadding', 'Ġcapacity', 'Ġat', 'Ġa', 'Ġrapid', 'Ġpace', 'Ġand', 'Ġcurrently', 'Ġrepresents', 'Ġ21', '%', 'Ġof', 'Ġthe', 'Ġtotal', 'Ġinstalled', 'Ġcapacity', '.', 'Ġthe', 'Ġremaining', 'Ġcapacity', 'Ġis', 'Ġnuclear', 'Ġ2', '%', 'Ġand', 'Ġhydro', 'Ġ13', '%.']\n",
      " \tpassage_offsets: [0, 4, 10, 17, 20, 23, 26, 29, 38, 41, 45, 47, 50, 52, 57, 65, 67, 77, 81, 89, 95, 97, 103, 113, 122, 125, 128, 130, 137, 139, 141, 146, 150, 154, 155, 156, 158, 161, 167, 169, 171, 174, 182, 192, 194, 204, 211, 214, 221, 230, 233, 235, 241, 246, 250, 260, 271, 273, 275, 278, 282, 288, 298, 306, 308, 312, 322, 331, 334, 342, 343, 345, 349, 355, 357]\n",
      " \tpassage_start_of_word: [1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0]\n",
      " \tquestion_tokens: ['What', 'Ġis', 'Ġthe', 'Ġannual', 'Ġtotal', 'Ġproduction', 'Ġfrom', 'Ġcoal', '?']\n",
      " \tquestion_offsets: [0, 5, 8, 12, 19, 25, 36, 41, 45]\n",
      " \tquestion_start_of_word: [1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      " \tanswers: []\n",
      " \tdocument_offsets: [0, 4, 10, 17, 20, 23, 26, 29, 38, 41, 45, 47, 50, 52, 57, 65, 67, 77, 81, 89, 95, 97, 103, 113, 122, 125, 128, 130, 137, 139, 141, 146, 150, 154, 155, 156, 158, 161, 167, 169, 171, 174, 182, 192, 194, 204, 211, 214, 221, 230, 233, 235, 241, 246, 250, 260, 271, 273, 275, 278, 282, 288, 298, 306, 308, 312, 322, 331, 334, 342, 343, 345, 349, 355, 357]\n",
      "Features: \n",
      " \tinput_ids: [0, 2264, 16, 5, 1013, 746, 931, 31, 4051, 116, 2, 2, 627, 476, 1293, 11, 9473, 493, 16, 14092, 9, 4051, 6, 1123, 6, 13575, 18986, 6, 8741, 8, 1748, 1007, 4, 746, 5923, 2148, 25, 9, 263, 47153, 1105, 6, 199, 21, 41091, 821, 605, 6, 9, 61, 4430, 207, 16, 17210, 2706, 4, 8741, 1007, 16, 1271, 2148, 23, 10, 6379, 2877, 8, 855, 3372, 733, 207, 9, 5, 746, 5923, 2148, 4, 5, 2405, 2148, 16, 1748, 132, 207, 8, 13575, 508, 2153, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " \tanswer_type_ids: [0]\n",
      " \tpassage_start_t: 0\n",
      " \tstart_of_word: [0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tlabels: [[ 0  0]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]]\n",
      " \tid: [3, 0, 0]\n",
      " \tseq_2_start_t: 12\n",
      "_____________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/14/2020 22:43:37 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 6-1-0\n",
      "Clear Text: \n",
      " \tpassage_text: aes also owns an 89% economic interest in the st. nikola wind farm with 156 mw of installed capacity. through december 31, 2018, the\n",
      " \tquestion_text: What is the total installed capacity from lignite (brown coal)?\n",
      " \tpassage_id: 0\n",
      " \tanswers: []\n",
      "Tokenized: \n",
      " \tpassage_start_t: 0\n",
      " \tpassage_tokens: ['a', 'es', 'Ġalso', 'Ġowns', 'Ġan', 'Ġ89', '%', 'Ġeconomic', 'Ġinterest', 'Ġin', 'Ġthe', 'Ġst', '.', 'Ġn', 'ik', 'ola', 'Ġwind', 'Ġfarm', 'Ġwith', 'Ġ156', 'Ġm', 'w', 'Ġof', 'Ġinstalled', 'Ġcapacity', '.', 'Ġthrough', 'Ġde', 'cember', 'Ġ31', ',', 'Ġ2018', ',', 'Ġthe']\n",
      " \tpassage_offsets: [0, 1, 4, 9, 14, 17, 19, 21, 30, 39, 42, 46, 48, 50, 51, 53, 57, 62, 67, 72, 76, 77, 79, 82, 92, 100, 102, 110, 112, 119, 121, 123, 127, 129]\n",
      " \tpassage_start_of_word: [1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1]\n",
      " \tquestion_tokens: ['What', 'Ġis', 'Ġthe', 'Ġtotal', 'Ġinstalled', 'Ġcapacity', 'Ġfrom', 'Ġl', 'ign', 'ite', 'Ġ(', 'brown', 'Ġcoal', ')?']\n",
      " \tquestion_offsets: [0, 5, 8, 12, 18, 28, 37, 42, 43, 46, 50, 51, 57, 61]\n",
      " \tquestion_start_of_word: [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0]\n",
      " \tanswers: []\n",
      " \tdocument_offsets: [0, 1, 4, 9, 14, 17, 19, 21, 30, 39, 42, 46, 48, 50, 51, 53, 57, 62, 67, 72, 76, 77, 79, 82, 92, 100, 102, 110, 112, 119, 121, 123, 127, 129]\n",
      "Features: \n",
      " \tinput_ids: [0, 2264, 16, 5, 746, 5923, 2148, 31, 784, 4932, 1459, 36, 31876, 4051, 26610, 2, 2, 102, 293, 67, 1831, 41, 8572, 207, 776, 773, 11, 5, 1690, 4, 295, 967, 3019, 2508, 3380, 19, 25664, 475, 605, 9, 5923, 2148, 4, 149, 263, 47153, 1105, 6, 199, 6, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " \tanswer_type_ids: [0]\n",
      " \tpassage_start_t: 0\n",
      " \tstart_of_word: [0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tlabels: [[ 0  0]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]]\n",
      " \tid: [6, 1, 0]\n",
      " \tseq_2_start_t: 17\n",
      "_____________________________________________________\n",
      "Preprocessing Dataset /model_pipeline/model_pipeline/data/train_split_esg_kpi_train_13-10-2020.json: 100%|██████████| 783/783 [00:05<00:00, 153.17 Dicts/s]\n",
      "10/14/2020 22:43:42 - INFO - farm.data_handler.data_silo -   Loading dev set from: /model_pipeline/model_pipeline/data/dev_split_esg_kpi_train_13-10-2020.json\n",
      "10/14/2020 22:43:42 - INFO - farm.data_handler.data_silo -   Got ya 15 parallel workers to convert 196 dictionaries to pytorch datasets (chunksize = 4)...\n",
      "10/14/2020 22:43:42 - INFO - farm.data_handler.data_silo -    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0 \n",
      "10/14/2020 22:43:42 - INFO - farm.data_handler.data_silo -   /w\\  /|\\  /|\\  /|\\  /|\\  /w\\  /w\\  /|\\  /|\\  /w\\  /w\\  /w\\  /w\\  /w\\  /w\\\n",
      "10/14/2020 22:43:42 - INFO - farm.data_handler.data_silo -   / \\  /'\\  /'\\  /'\\  /'\\  / \\  / \\  /'\\  /'\\  / \\  / \\  / \\  /'\\  /'\\  /'\\\n",
      "10/14/2020 22:43:42 - INFO - farm.data_handler.data_silo -                               \n",
      "Preprocessing Dataset /model_pipeline/model_pipeline/data/dev_split_esg_kpi_train_13-10-2020.json:   0%|          | 0/196 [00:00<?, ? Dicts/s]10/14/2020 22:43:42 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
      "10/14/2020 22:43:42 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 0-2-0\n",
      "Clear Text: \n",
      " \tpassage_text: hydro assets in panama have ppas with distribution companies up to december 2030 for a total contracted capacity of 350 mw. thermal\n",
      " \tquestion_text: What is the total installed capacity from hard coal?\n",
      " \tpassage_id: 0\n",
      " \tanswers: []\n",
      "Tokenized: \n",
      " \tpassage_start_t: 0\n",
      " \tpassage_tokens: ['hyd', 'ro', 'Ġassets', 'Ġin', 'Ġpan', 'ama', 'Ġhave', 'Ġpp', 'as', 'Ġwith', 'Ġdistribution', 'Ġcompanies', 'Ġup', 'Ġto', 'Ġde', 'cember', 'Ġ2030', 'Ġfor', 'Ġa', 'Ġtotal', 'Ġcontracted', 'Ġcapacity', 'Ġof', 'Ġ350', 'Ġm', 'w', '.', 'Ġthermal']\n",
      " \tpassage_offsets: [0, 3, 6, 13, 16, 19, 23, 28, 30, 33, 38, 51, 61, 64, 67, 69, 76, 81, 85, 87, 93, 104, 113, 116, 120, 121, 122, 124]\n",
      " \tpassage_start_of_word: [1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1]\n",
      " \tquestion_tokens: ['What', 'Ġis', 'Ġthe', 'Ġtotal', 'Ġinstalled', 'Ġcapacity', 'Ġfrom', 'Ġhard', 'Ġcoal', '?']\n",
      " \tquestion_offsets: [0, 5, 8, 12, 18, 28, 37, 42, 47, 51]\n",
      " \tquestion_start_of_word: [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      " \tanswers: []\n",
      " \tdocument_offsets: [0, 3, 6, 13, 16, 19, 23, 28, 30, 33, 38, 51, 61, 64, 67, 69, 76, 81, 85, 87, 93, 104, 113, 116, 120, 121, 122, 124]\n",
      "Features: \n",
      " \tinput_ids: [0, 2264, 16, 5, 746, 5923, 2148, 31, 543, 4051, 116, 2, 2, 30420, 1001, 1781, 11, 5730, 2583, 33, 33906, 281, 19, 3854, 451, 62, 7, 263, 47153, 12060, 13, 10, 746, 13196, 2148, 9, 10088, 475, 605, 4, 17210, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " \tanswer_type_ids: [0]\n",
      " \tpassage_start_t: 0\n",
      " \tstart_of_word: [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tlabels: [[ 0  0]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]]\n",
      " \tid: [0, 2, 0]\n",
      " \tseq_2_start_t: 13\n",
      "_____________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/14/2020 22:43:42 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 1-0-0\n",
      "Clear Text: \n",
      " \tpassage_text: regulatory framework and market structure — electricity supply in colombia is concentrated in one main system, the sin, which encompasses one-third of colombia's territory, providing electricity to 97% of the country's population. the sin's installed capacity, primarily hydroelectric 68% and thermal 31%, totaled 17,392 mw as of december 31, 2018. the marked seasonal variations in colombia's hydrology result in price volatility in the short-term market. in 2018, 84% of total energy demand was supplied by hydroelectric plants.\n",
      " \tquestion_text: What is the total installed capacity from coal?\n",
      " \tpassage_id: 0\n",
      " \tanswers: []\n",
      "Tokenized: \n",
      " \tpassage_start_t: 0\n",
      " \tpassage_tokens: ['reg', 'ulatory', 'Ġframework', 'Ġand', 'Ġmarket', 'Ġstructure', 'ĠâĢĶ', 'Ġelectricity', 'Ġsupply', 'Ġin', 'Ġcol', 'omb', 'ia', 'Ġis', 'Ġconcentrated', 'Ġin', 'Ġone', 'Ġmain', 'Ġsystem', ',', 'Ġthe', 'Ġsin', ',', 'Ġwhich', 'Ġencompasses', 'Ġone', '-', 'third', 'Ġof', 'Ġcol', 'omb', 'ia', \"'s\", 'Ġterritory', ',', 'Ġproviding', 'Ġelectricity', 'Ġto', 'Ġ97', '%', 'Ġof', 'Ġthe', 'Ġcountry', \"'s\", 'Ġpopulation', '.', 'Ġthe', 'Ġsin', \"'s\", 'Ġinstalled', 'Ġcapacity', ',', 'Ġprimarily', 'Ġhydro', 'electric', 'Ġ68', '%', 'Ġand', 'Ġthermal', 'Ġ31', '%,', 'Ġtotaled', 'Ġ17', ',', '392', 'Ġm', 'w', 'Ġas', 'Ġof', 'Ġde', 'cember', 'Ġ31', ',', 'Ġ2018', '.', 'Ġthe', 'Ġmarked', 'Ġseasonal', 'Ġvariations', 'Ġin', 'Ġcol', 'omb', 'ia', \"'s\", 'Ġhyd', 'rology', 'Ġresult', 'Ġin', 'Ġprice', 'Ġvolatility', 'Ġin', 'Ġthe', 'Ġshort', '-', 'term', 'Ġmarket', '.', 'Ġin', 'Ġ2018', ',', 'Ġ84', '%', 'Ġof', 'Ġtotal', 'Ġenergy', 'Ġdemand', 'Ġwas', 'Ġsupplied', 'Ġby', 'Ġhydro', 'electric', 'Ġplants', '.']\n",
      " \tpassage_offsets: [0, 3, 11, 21, 25, 32, 42, 44, 56, 63, 66, 69, 72, 75, 78, 91, 94, 98, 103, 109, 111, 115, 118, 120, 126, 138, 141, 142, 148, 151, 154, 157, 159, 162, 171, 173, 183, 195, 198, 200, 202, 205, 209, 216, 219, 229, 231, 235, 238, 241, 251, 259, 261, 271, 276, 285, 287, 289, 293, 301, 303, 306, 314, 316, 317, 321, 322, 324, 327, 330, 332, 339, 341, 343, 347, 349, 353, 360, 369, 380, 383, 386, 389, 391, 394, 397, 404, 411, 414, 420, 431, 434, 438, 443, 444, 449, 455, 457, 460, 464, 466, 468, 470, 473, 479, 486, 493, 497, 506, 509, 514, 523, 529]\n",
      " \tpassage_start_of_word: [1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      " \tquestion_tokens: ['What', 'Ġis', 'Ġthe', 'Ġtotal', 'Ġinstalled', 'Ġcapacity', 'Ġfrom', 'Ġcoal', '?']\n",
      " \tquestion_offsets: [0, 5, 8, 12, 18, 28, 37, 42, 46]\n",
      " \tquestion_start_of_word: [1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      " \tanswers: []\n",
      " \tdocument_offsets: [0, 3, 11, 21, 25, 32, 42, 44, 56, 63, 66, 69, 72, 75, 78, 91, 94, 98, 103, 109, 111, 115, 118, 120, 126, 138, 141, 142, 148, 151, 154, 157, 159, 162, 171, 173, 183, 195, 198, 200, 202, 205, 209, 216, 219, 229, 231, 235, 238, 241, 251, 259, 261, 271, 276, 285, 287, 289, 293, 301, 303, 306, 314, 316, 317, 321, 322, 324, 327, 330, 332, 339, 341, 343, 347, 349, 353, 360, 369, 380, 383, 386, 389, 391, 394, 397, 404, 411, 414, 420, 431, 434, 438, 443, 444, 449, 455, 457, 460, 464, 466, 468, 470, 473, 479, 486, 493, 497, 506, 509, 514, 523, 529]\n",
      "Features: \n",
      " \tinput_ids: [0, 2264, 16, 5, 746, 5923, 2148, 31, 4051, 116, 2, 2, 4950, 25911, 7208, 8, 210, 3184, 93, 4382, 1787, 11, 11311, 5223, 493, 16, 15450, 11, 65, 1049, 467, 6, 5, 10272, 6, 61, 27747, 65, 12, 12347, 9, 11311, 5223, 493, 18, 4284, 6, 1976, 4382, 7, 8783, 207, 9, 5, 247, 18, 1956, 4, 5, 10272, 18, 5923, 2148, 6, 4212, 13575, 18986, 5595, 207, 8, 17210, 1105, 4234, 15137, 601, 6, 36350, 475, 605, 25, 9, 263, 47153, 1105, 6, 199, 4, 5, 4760, 10175, 18746, 11, 11311, 5223, 493, 18, 13075, 27895, 898, 11, 425, 4975, 11, 5, 765, 12, 1279, 210, 4, 11, 199, 6, 7994, 207, 9, 746, 1007, 1077, 21, 12359, 30, 13575, 18986, 3451, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " \tanswer_type_ids: [0]\n",
      " \tpassage_start_t: 0\n",
      " \tstart_of_word: [0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tlabels: [[ 0  0]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]]\n",
      " \tid: [1, 0, 0]\n",
      " \tseq_2_start_t: 12\n",
      "_____________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing Dataset /model_pipeline/model_pipeline/data/dev_split_esg_kpi_train_13-10-2020.json: 100%|██████████| 196/196 [00:03<00:00, 56.88 Dicts/s]\n",
      "10/14/2020 22:43:45 - INFO - farm.data_handler.data_silo -   No test set is being loaded\n",
      "10/14/2020 22:43:45 - INFO - farm.data_handler.data_silo -   Examples in train: 1911\n",
      "10/14/2020 22:43:45 - INFO - farm.data_handler.data_silo -   Examples in dev  : 496\n",
      "10/14/2020 22:43:45 - INFO - farm.data_handler.data_silo -   Examples in test : 0\n",
      "10/14/2020 22:43:45 - INFO - farm.data_handler.data_silo -   \n",
      "10/14/2020 22:43:45 - INFO - farm.data_handler.data_silo -   Longest sequence length observed after clipping:     384\n",
      "10/14/2020 22:43:45 - INFO - farm.data_handler.data_silo -   Average sequence length after clipping: 80.80115122972266\n",
      "10/14/2020 22:43:45 - INFO - farm.data_handler.data_silo -   Proportion clipped:      0.006802721088435374\n",
      "10/14/2020 22:44:05 - WARNING - farm.modeling.prediction_head -   Some unused parameters are passed to the QuestionAnsweringHead. Might not be a problem. Params: {\"loss_ignore_index\": -1}\n",
      "10/14/2020 22:44:05 - INFO - farm.modeling.prediction_head -   Prediction head initialized with size [1024, 2]\n",
      "10/14/2020 22:44:19 - WARNING - farm.modeling.language_model -   Could not automatically detect from language model name what language it is. \n",
      "\t We guess it's an *ENGLISH* model ... \n",
      "\t If not: Init the language model by supplying the 'language' param.\n",
      "10/14/2020 22:44:23 - INFO - farm.modeling.optimization -   Loading optimizer `TransformersAdamW`: '{'correct_bias': False, 'weight_decay': 0.01, 'lr': 2e-05}'\n",
      "10/14/2020 22:44:23 - INFO - farm.modeling.optimization -   Using scheduler 'get_linear_schedule_with_warmup'\n",
      "10/14/2020 22:44:23 - INFO - farm.modeling.optimization -   Loading schedule `get_linear_schedule_with_warmup`: '{'num_warmup_steps': 47.6, 'num_training_steps': 476}'\n",
      "10/14/2020 22:44:23 - INFO - farm.train -   \n",
      " \n",
      "\n",
      "          &&& &&  & &&             _____                   _             \n",
      "      && &\\/&\\|& ()|/ @, &&       / ____|                 (_)            \n",
      "      &\\/(/&/&||/& /_/)_&/_&     | |  __ _ __ _____      ___ _ __   __ _ \n",
      "   &() &\\/&|()|/&\\/ '%\" & ()     | | |_ | '__/ _ \\ \\ /\\ / / | '_ \\ / _` |\n",
      "  &_\\_&&_\\ |& |&&/&__%_/_& &&    | |__| | | | (_) \\ V  V /| | | | | (_| |\n",
      "&&   && & &| &| /& & % ()& /&&    \\_____|_|  \\___/ \\_/\\_/ |_|_| |_|\\__, |\n",
      " ()&_---()&\\&\\|&&-&&--%---()~                                       __/ |\n",
      "     &&     \\|||                                                   |___/\n",
      "             |||\n",
      "             |||\n",
      "             |||\n",
      "       , -=-~  .-^- _\n",
      "              `\n",
      "\n",
      "Train epoch 0/3 (Cur. train loss: 0.1636):  84%|████████▎ | 400/478 [03:34<00:40,  1.92it/s]\n",
      "Evaluating:   0%|          | 0/124 [00:00<?, ?it/s]\u001b[A\n",
      "Evaluating:  46%|████▌     | 57/124 [00:10<00:11,  5.68it/s]\u001b[A\n",
      "Evaluating: 100%|██████████| 124/124 [00:21<00:00,  5.68it/s]\u001b[A\n",
      "10/14/2020 22:48:19 - INFO - farm.eval -   \n",
      "\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "***************************************************\n",
      "***** EVALUATION | DEV SET | AFTER 400 BATCHES *****\n",
      "***************************************************\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "10/14/2020 22:48:19 - INFO - farm.eval -   \n",
      " _________ question_answering _________\n",
      "10/14/2020 22:48:19 - INFO - farm.eval -   loss: 0.35963449187480484\n",
      "10/14/2020 22:48:19 - INFO - farm.eval -   task_name: question_answering\n",
      "10/14/2020 22:48:19 - INFO - farm.eval -   EM: 0.8904665314401623\n",
      "10/14/2020 22:48:19 - INFO - farm.eval -   f1: 0.9034547921685494\n",
      "10/14/2020 22:48:19 - INFO - farm.eval -   top_n_accuracy: 0.9391480730223124\n",
      "10/14/2020 22:48:19 - INFO - farm.eval -   report: \n",
      " Not Implemented\n",
      "Train epoch 0/3 (Cur. train loss: 0.4870): 100%|██████████| 478/478 [04:36<00:00,  1.73it/s]\n",
      "Train epoch 1/3 (Cur. train loss: 0.1572):  67%|██████▋   | 322/478 [02:48<01:21,  1.91it/s]\n",
      "Evaluating:   0%|          | 0/124 [00:00<?, ?it/s]\u001b[A\n",
      "Evaluating:  46%|████▌     | 57/124 [00:10<00:11,  5.67it/s]\u001b[A\n",
      "Evaluating: 100%|██████████| 124/124 [00:21<00:00,  5.68it/s]\u001b[A\n",
      "10/14/2020 22:52:11 - INFO - farm.eval -   \n",
      "\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "***************************************************\n",
      "***** EVALUATION | DEV SET | AFTER 800 BATCHES *****\n",
      "***************************************************\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "10/14/2020 22:52:11 - INFO - farm.eval -   \n",
      " _________ question_answering _________\n",
      "10/14/2020 22:52:11 - INFO - farm.eval -   loss: 0.21250644177604455\n",
      "10/14/2020 22:52:11 - INFO - farm.eval -   task_name: question_answering\n",
      "10/14/2020 22:52:11 - INFO - farm.eval -   EM: 0.920892494929006\n",
      "10/14/2020 22:52:11 - INFO - farm.eval -   f1: 0.9384604412985855\n",
      "10/14/2020 22:52:11 - INFO - farm.eval -   top_n_accuracy: 0.9776876267748479\n",
      "10/14/2020 22:52:11 - INFO - farm.eval -   report: \n",
      " Not Implemented\n",
      "Train epoch 1/3 (Cur. train loss: 0.0000): 100%|██████████| 478/478 [04:32<00:00,  1.76it/s]\n",
      "Train epoch 2/3 (Cur. train loss: 0.0059):  51%|█████     | 244/478 [02:07<01:59,  1.96it/s]\n",
      "Evaluating:   0%|          | 0/124 [00:00<?, ?it/s]\u001b[A\n",
      "Evaluating:  40%|████      | 50/124 [00:10<00:14,  4.95it/s]\u001b[A\n",
      "Evaluating: 100%|██████████| 124/124 [00:23<00:00,  5.29it/s]\u001b[A\n",
      "10/14/2020 22:56:04 - INFO - farm.eval -   \n",
      "\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "***************************************************\n",
      "***** EVALUATION | DEV SET | AFTER 1200 BATCHES *****\n",
      "***************************************************\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "10/14/2020 22:56:04 - INFO - farm.eval -   \n",
      " _________ question_answering _________\n",
      "10/14/2020 22:56:04 - INFO - farm.eval -   loss: 0.275140286765643\n",
      "10/14/2020 22:56:04 - INFO - farm.eval -   task_name: question_answering\n",
      "10/14/2020 22:56:04 - INFO - farm.eval -   EM: 0.9391480730223124\n",
      "10/14/2020 22:56:04 - INFO - farm.eval -   f1: 0.9477234676524737\n",
      "10/14/2020 22:56:04 - INFO - farm.eval -   top_n_accuracy: 0.973630831643002\n",
      "10/14/2020 22:56:04 - INFO - farm.eval -   report: \n",
      " Not Implemented\n",
      "Train epoch 2/3 (Cur. train loss: 0.0031): 100%|██████████| 478/478 [04:33<00:00,  1.75it/s]\n",
      "Train epoch 3/3 (Cur. train loss: 0.0011):  35%|███▍      | 166/478 [01:26<02:44,  1.90it/s]\n",
      "Evaluating:   0%|          | 0/124 [00:00<?, ?it/s]\u001b[A\n",
      "Evaluating:  35%|███▌      | 44/124 [00:10<00:18,  4.26it/s]\u001b[A\n",
      "Evaluating: 100%|██████████| 124/124 [00:25<00:00,  4.85it/s][A\n",
      "10/14/2020 22:59:58 - INFO - farm.eval -   \n",
      "\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "***************************************************\n",
      "***** EVALUATION | DEV SET | AFTER 1600 BATCHES *****\n",
      "***************************************************\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "10/14/2020 22:59:58 - INFO - farm.eval -   \n",
      " _________ question_answering _________\n",
      "10/14/2020 22:59:59 - INFO - farm.eval -   loss: 0.24522944951460368\n",
      "10/14/2020 22:59:59 - INFO - farm.eval -   task_name: question_answering\n",
      "10/14/2020 22:59:59 - INFO - farm.eval -   EM: 0.9513184584178499\n",
      "10/14/2020 22:59:59 - INFO - farm.eval -   f1: 0.9624668312647852\n",
      "10/14/2020 22:59:59 - INFO - farm.eval -   top_n_accuracy: 0.9776876267748479\n",
      "10/14/2020 22:59:59 - INFO - farm.eval -   report: \n",
      " Not Implemented\n",
      "Train epoch 3/3 (Cur. train loss: 0.0000): 100%|██████████| 478/478 [04:35<00:00,  1.74it/s]\n",
      "Evaluating: 100%|██████████| 124/124 [00:22<00:00,  5.56it/s]\n",
      "10/14/2020 23:03:03 - INFO - farm.eval -   \n",
      "\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "***************************************************\n",
      "***** EVALUATION | DEV SET | AFTER 124 BATCHES *****\n",
      "***************************************************\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/14/2020 23:03:03 - INFO - farm.eval -   \n",
      " _________ question_answering _________\n",
      "10/14/2020 23:03:03 - INFO - farm.eval -   loss: 0.20515313238739313\n",
      "10/14/2020 23:03:03 - INFO - farm.eval -   task_name: question_answering\n",
      "10/14/2020 23:03:03 - INFO - farm.eval -   EM: 0.9452332657200812\n",
      "10/14/2020 23:03:04 - INFO - farm.eval -   f1: 0.9597578216239473\n",
      "10/14/2020 23:03:04 - INFO - farm.eval -   top_n_accuracy: 0.9756592292089249\n",
      "10/14/2020 23:03:04 - INFO - farm.eval -   report: \n",
      " Not Implemented\n",
      "10/14/2020 23:03:04 - INFO - model_pipeline.qa_farm_trainer -   Extended Results:\n",
      "10/14/2020 23:03:04 - INFO - model_pipeline.qa_farm_trainer -   {'TN': 380, 'FP': 2, 'FN': 4, 'TP': 107, 'relaxed_f1_answerable': 0.8648648648648649, 'em_answerable': 0.7747747747747747, 'f1_answerable': 0.8392847392847392}\n",
      "10/14/2020 23:03:16 - INFO - model_pipeline.farm_trainer -   Trained model saved to /model_pipeline/model_pipeline/saved_models/test_farm\n",
      "10/14/2020 23:03:16 - INFO - model_pipeline.farm_trainer -   Processor vocabulary saved to /model_pipeline/model_pipeline/saved_models/test_farm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9597578216239473"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "farm_trainer.run(metric=\"f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the training process, the model and the processor vocabulary are saved into the directory `file_config.saved_models_dir`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1389660\r\n",
      "drwxr-xr-x 2 root root       4096 Oct 14 20:31 .\r\n",
      "drwxr-xr-x 1 root root       4096 Oct 14 20:31 ..\r\n",
      "-rw-r--r-- 1 root root 1421605239 Oct 14 23:03 language_model.bin\r\n",
      "-rw-r--r-- 1 root root        572 Oct 14 23:03 language_model_config.json\r\n",
      "-rw-r--r-- 1 root root     456318 Oct 14 23:03 merges.txt\r\n",
      "-rw-r--r-- 1 root root       9473 Oct 14 23:03 prediction_head_0.bin\r\n",
      "-rw-r--r-- 1 root root        705 Oct 14 23:03 prediction_head_0_config.json\r\n",
      "-rw-r--r-- 1 root root        849 Oct 14 23:03 processor_config.json\r\n",
      "-rw-r--r-- 1 root root        150 Oct 14 23:03 special_tokens_map.json\r\n",
      "-rw-r--r-- 1 root root        564 Oct 14 23:03 tokenizer_config.json\r\n",
      "-rw-r--r-- 1 root root     898822 Oct 14 23:03 vocab.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls -al $file_config.saved_models_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the developement dataset at `file_config.dev_filename`. This dataset has not been seen by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/model_pipeline/model_pipeline/data/dev_split_esg_kpi_train_13-10-2020.json'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_config.dev_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the saved model and test it on some real examples.<br><br>\n",
    "First let's load the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/14/2020 23:04:38 - INFO - farm.utils -   device: cuda n_gpu: 1, distributed training: False, automatic mixed precision training: None\n",
      "10/14/2020 23:04:52 - INFO - farm.modeling.adaptive_model -   Found files for loading 1 prediction heads\n",
      "10/14/2020 23:04:52 - WARNING - farm.modeling.prediction_head -   Some unused parameters are passed to the QuestionAnsweringHead. Might not be a problem. Params: {\"training\": false, \"num_labels\": 2, \"ph_output_type\": \"per_token_squad\", \"model_type\": \"span_classification\", \"config\": {\"training\": true, \"layer_dims\": [1024, 2], \"num_labels\": 2, \"ph_output_type\": \"per_token_squad\", \"model_type\": \"span_classification\", \"task_name\": \"question_answering\", \"no_ans_boost\": 0.0, \"context_window_size\": 100, \"n_best\": 5, \"n_best_per_sample\": 1, \"name\": \"QuestionAnsweringHead\"}, \"label_tensor_name\": \"question_answering_label_ids\", \"label_list\": [\"start_token\", \"end_token\"], \"metric\": \"squad\", \"name\": \"QuestionAnsweringHead\"}\n",
      "10/14/2020 23:04:52 - INFO - farm.modeling.prediction_head -   Prediction head initialized with size [1024, 2]\n",
      "10/14/2020 23:04:52 - INFO - farm.modeling.prediction_head -   Loading prediction head from /model_pipeline/model_pipeline/saved_models/test_farm/prediction_head_0.bin\n",
      "10/14/2020 23:04:52 - INFO - farm.data_handler.processor -   Initialized processor without tasks. Supply `metric` and `label_list` to the constructor for using the default task or add a custom task later via processor.add_task()\n",
      "10/14/2020 23:04:52 - INFO - farm.utils -   device: cuda n_gpu: 1, distributed training: False, automatic mixed precision training: None\n",
      "10/14/2020 23:04:53 - INFO - farm.infer -   Got ya 15 parallel workers to do inference ...\n",
      "10/14/2020 23:04:53 - INFO - farm.infer -    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0 \n",
      "10/14/2020 23:04:53 - INFO - farm.infer -   /w\\  /w\\  /w\\  /w\\  /w\\  /w\\  /w\\  /|\\  /w\\  /w\\  /w\\  /w\\  /w\\  /w\\  /|\\\n",
      "10/14/2020 23:04:53 - INFO - farm.infer -   /'\\  / \\  /'\\  /'\\  / \\  / \\  /'\\  /'\\  /'\\  /'\\  /'\\  /'\\  / \\  /'\\  /'\\\n",
      "10/14/2020 23:04:53 - INFO - farm.infer -                               \n",
      "10/14/2020 23:04:53 - WARNING - farm.infer -   QAInferencer always has task_type='question_answering' even if another value is provided to Inferencer.load() or QAInferencer()\n",
      "10/14/2020 23:06:03 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
      "10/14/2020 23:06:03 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 0-0-0\n",
      "Clear Text: \n",
      " \tpassage_text: the paris agreement on climate change drafted in 2015 aims to reduce worldwide emissions of greenhouse \n",
      "gases to a level intended to limit a rise in global temperatures to below 2 degrees or, better still,\n",
      "to below 1.5 degrees. verbund’s target of reducing greenhouse gas emissions by 90% measured beginning from \n",
      "the basis year 2011 5 million tonnes co2e until 2021 includes scope 1, scope 2 market- based and parts of scope 3 emissions \n",
      "for energy and air travel. the science based targets initiative validated this goal as science-based in october 2016, \n",
      "i.e. it meets global standards. according to current planning, the target can be achieved. \n",
      "however, if the grid operator requires higher generation volumes \n",
      "\n",
      " \tquestion_text: What is the target year for climate commitment?\n",
      " \tpassage_id: 0\n",
      " \tanswers: []\n",
      "Tokenized: \n",
      " \tpassage_start_t: 0\n",
      " \tpassage_tokens: ['the', 'Ġpar', 'is', 'Ġagreement', 'Ġon', 'Ġclimate', 'Ġchange', 'Ġdrafted', 'Ġin', 'Ġ2015', 'Ġaims', 'Ġto', 'Ġreduce', 'Ġworldwide', 'Ġemissions', 'Ġof', 'Ġgreenhouse', 'Ġgases', 'Ġto', 'Ġa', 'Ġlevel', 'Ġintended', 'Ġto', 'Ġlimit', 'Ġa', 'Ġrise', 'Ġin', 'Ġglobal', 'Ġtemperatures', 'Ġto', 'Ġbelow', 'Ġ2', 'Ġdegrees', 'Ġor', ',', 'Ġbetter', 'Ġstill', ',', 'Ġto', 'Ġbelow', 'Ġ1', '.', '5', 'Ġdegrees', '.', 'Ġverb', 'und', 'âĢ', 'Ļ', 's', 'Ġtarget', 'Ġof', 'Ġreducing', 'Ġgreenhouse', 'Ġgas', 'Ġemissions', 'Ġby', 'Ġ90', '%', 'Ġmeasured', 'Ġbeginning', 'Ġfrom', 'Ġthe', 'Ġbasis', 'Ġyear', 'Ġ2011', 'Ġ5', 'Ġmillion', 'Ġtonnes', 'Ġco', '2', 'e', 'Ġuntil', 'Ġ2021', 'Ġincludes', 'Ġscope', 'Ġ1', ',', 'Ġscope', 'Ġ2', 'Ġmarket', '-', 'Ġbased', 'Ġand', 'Ġparts', 'Ġof', 'Ġscope', 'Ġ3', 'Ġemissions', 'Ġfor', 'Ġenergy', 'Ġand', 'Ġair', 'Ġtravel', '.', 'Ġthe', 'Ġscience', 'Ġbased', 'Ġtargets', 'Ġinitiative', 'Ġvalidated', 'Ġthis', 'Ġgoal', 'Ġas', 'Ġscience', '-', 'based', 'Ġin', 'Ġoct', 'ober', 'Ġ2016', ',', 'Ġi', '.', 'e', '.', 'Ġit', 'Ġmeets', 'Ġglobal', 'Ġstandards', '.', 'Ġaccording', 'Ġto', 'Ġcurrent', 'Ġplanning', ',', 'Ġthe', 'Ġtarget', 'Ġcan', 'Ġbe', 'Ġachieved', '.', 'Ġhowever', ',', 'Ġif', 'Ġthe', 'Ġgrid', 'Ġoperator', 'Ġrequires', 'Ġhigher', 'Ġgeneration', 'Ġvolumes']\n",
      " \tpassage_offsets: [0, 4, 7, 10, 20, 23, 31, 38, 46, 49, 54, 59, 62, 69, 79, 89, 92, 104, 110, 113, 115, 121, 130, 133, 139, 141, 146, 149, 156, 169, 172, 178, 180, 188, 190, 192, 199, 204, 206, 209, 215, 216, 217, 219, 226, 228, 232, 235, 237, 238, 238, 245, 248, 257, 268, 272, 282, 285, 287, 289, 298, 308, 314, 318, 324, 329, 334, 336, 344, 351, 353, 354, 356, 362, 367, 376, 382, 383, 385, 391, 393, 399, 401, 407, 411, 417, 420, 426, 428, 439, 443, 450, 454, 458, 464, 466, 470, 478, 484, 492, 503, 513, 518, 523, 526, 533, 534, 540, 543, 546, 551, 555, 558, 559, 560, 561, 563, 566, 572, 579, 588, 590, 600, 603, 611, 619, 621, 625, 632, 636, 639, 647, 650, 657, 659, 662, 666, 671, 680, 689, 696, 707]\n",
      " \tpassage_start_of_word: [1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      " \tquestion_tokens: ['What', 'Ġis', 'Ġthe', 'Ġtarget', 'Ġyear', 'Ġfor', 'Ġclimate', 'Ġcommitment', '?']\n",
      " \tquestion_offsets: [0, 5, 8, 12, 19, 24, 28, 36, 46]\n",
      " \tquestion_start_of_word: [1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      " \tanswers: []\n",
      " \tdocument_offsets: [0, 4, 7, 10, 20, 23, 31, 38, 46, 49, 54, 59, 62, 69, 79, 89, 92, 104, 110, 113, 115, 121, 130, 133, 139, 141, 146, 149, 156, 169, 172, 178, 180, 188, 190, 192, 199, 204, 206, 209, 215, 216, 217, 219, 226, 228, 232, 235, 237, 238, 238, 245, 248, 257, 268, 272, 282, 285, 287, 289, 298, 308, 314, 318, 324, 329, 334, 336, 344, 351, 353, 354, 356, 362, 367, 376, 382, 383, 385, 391, 393, 399, 401, 407, 411, 417, 420, 426, 428, 439, 443, 450, 454, 458, 464, 466, 470, 478, 484, 492, 503, 513, 518, 523, 526, 533, 534, 540, 543, 546, 551, 555, 558, 559, 560, 561, 563, 566, 572, 579, 588, 590, 600, 603, 611, 619, 621, 625, 632, 636, 639, 647, 650, 657, 659, 662, 666, 671, 680, 689, 696, 707]\n",
      "Features: \n",
      " \tinput_ids: [0, 2264, 16, 5, 1002, 76, 13, 2147, 2720, 116, 2, 2, 627, 2242, 354, 1288, 15, 2147, 464, 9559, 11, 570, 5026, 7, 1888, 3612, 5035, 9, 11832, 20038, 7, 10, 672, 3833, 7, 3000, 10, 1430, 11, 720, 3971, 7, 874, 132, 4176, 50, 6, 357, 202, 6, 7, 874, 112, 4, 245, 4176, 4, 33760, 3194, 17, 27, 29, 1002, 9, 4881, 11832, 1123, 5035, 30, 1814, 207, 9550, 1786, 31, 5, 1453, 76, 1466, 195, 153, 5657, 1029, 176, 242, 454, 8835, 1171, 7401, 112, 6, 7401, 132, 210, 12, 716, 8, 1667, 9, 7401, 155, 5035, 13, 1007, 8, 935, 1504, 4, 5, 2866, 716, 3247, 3893, 29548, 42, 724, 25, 2866, 12, 805, 11, 16874, 24761, 336, 6, 939, 4, 242, 4, 24, 6616, 720, 2820, 4, 309, 7, 595, 1884, 6, 5, 1002, 64, 28, 4824, 4, 959, 6, 114, 5, 7961, 5364, 3441, 723, 2706, 7267, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " \tanswer_type_ids: [0]\n",
      " \tpassage_start_t: 0\n",
      " \tstart_of_word: [0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tlabels: [[ 0  0]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]]\n",
      " \tid: [0, 0, 0]\n",
      " \tseq_2_start_t: 12\n",
      "_____________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/14/2020 23:06:03 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 0-0-0\n",
      "Clear Text: \n",
      " \tpassage_text: the paris agreement on climate change drafted in 2015 aims to reduce worldwide emissions of greenhouse \n",
      "gases to a level intended to limit a rise in global temperatures to below 2 degrees or, better still,\n",
      "to below 1.5 degrees. verbund’s target of reducing greenhouse gas emissions by 90% measured beginning from \n",
      "the basis year 2011 5 million tonnes co2e until 2021 includes scope 1, scope 2 market- based and parts of scope 3 emissions \n",
      "for energy and air travel. the science based targets initiative validated this goal as science-based in october 2016, \n",
      "i.e. it meets global standards. according to current planning, the target can be achieved. \n",
      "however, if the grid operator requires higher generation volumes \n",
      "\n",
      " \tquestion_text: What is the target year for climate commitment?\n",
      " \tpassage_id: 0\n",
      " \tanswers: []\n",
      "Tokenized: \n",
      " \tpassage_start_t: 0\n",
      " \tpassage_tokens: ['the', 'Ġpar', 'is', 'Ġagreement', 'Ġon', 'Ġclimate', 'Ġchange', 'Ġdrafted', 'Ġin', 'Ġ2015', 'Ġaims', 'Ġto', 'Ġreduce', 'Ġworldwide', 'Ġemissions', 'Ġof', 'Ġgreenhouse', 'Ġgases', 'Ġto', 'Ġa', 'Ġlevel', 'Ġintended', 'Ġto', 'Ġlimit', 'Ġa', 'Ġrise', 'Ġin', 'Ġglobal', 'Ġtemperatures', 'Ġto', 'Ġbelow', 'Ġ2', 'Ġdegrees', 'Ġor', ',', 'Ġbetter', 'Ġstill', ',', 'Ġto', 'Ġbelow', 'Ġ1', '.', '5', 'Ġdegrees', '.', 'Ġverb', 'und', 'âĢ', 'Ļ', 's', 'Ġtarget', 'Ġof', 'Ġreducing', 'Ġgreenhouse', 'Ġgas', 'Ġemissions', 'Ġby', 'Ġ90', '%', 'Ġmeasured', 'Ġbeginning', 'Ġfrom', 'Ġthe', 'Ġbasis', 'Ġyear', 'Ġ2011', 'Ġ5', 'Ġmillion', 'Ġtonnes', 'Ġco', '2', 'e', 'Ġuntil', 'Ġ2021', 'Ġincludes', 'Ġscope', 'Ġ1', ',', 'Ġscope', 'Ġ2', 'Ġmarket', '-', 'Ġbased', 'Ġand', 'Ġparts', 'Ġof', 'Ġscope', 'Ġ3', 'Ġemissions', 'Ġfor', 'Ġenergy', 'Ġand', 'Ġair', 'Ġtravel', '.', 'Ġthe', 'Ġscience', 'Ġbased', 'Ġtargets', 'Ġinitiative', 'Ġvalidated', 'Ġthis', 'Ġgoal', 'Ġas', 'Ġscience', '-', 'based', 'Ġin', 'Ġoct', 'ober', 'Ġ2016', ',', 'Ġi', '.', 'e', '.', 'Ġit', 'Ġmeets', 'Ġglobal', 'Ġstandards', '.', 'Ġaccording', 'Ġto', 'Ġcurrent', 'Ġplanning', ',', 'Ġthe', 'Ġtarget', 'Ġcan', 'Ġbe', 'Ġachieved', '.', 'Ġhowever', ',', 'Ġif', 'Ġthe', 'Ġgrid', 'Ġoperator', 'Ġrequires', 'Ġhigher', 'Ġgeneration', 'Ġvolumes']\n",
      " \tpassage_offsets: [0, 4, 7, 10, 20, 23, 31, 38, 46, 49, 54, 59, 62, 69, 79, 89, 92, 104, 110, 113, 115, 121, 130, 133, 139, 141, 146, 149, 156, 169, 172, 178, 180, 188, 190, 192, 199, 204, 206, 209, 215, 216, 217, 219, 226, 228, 232, 235, 237, 238, 238, 245, 248, 257, 268, 272, 282, 285, 287, 289, 298, 308, 314, 318, 324, 329, 334, 336, 344, 351, 353, 354, 356, 362, 367, 376, 382, 383, 385, 391, 393, 399, 401, 407, 411, 417, 420, 426, 428, 439, 443, 450, 454, 458, 464, 466, 470, 478, 484, 492, 503, 513, 518, 523, 526, 533, 534, 540, 543, 546, 551, 555, 558, 559, 560, 561, 563, 566, 572, 579, 588, 590, 600, 603, 611, 619, 621, 625, 632, 636, 639, 647, 650, 657, 659, 662, 666, 671, 680, 689, 696, 707]\n",
      " \tpassage_start_of_word: [1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      " \tquestion_tokens: ['What', 'Ġis', 'Ġthe', 'Ġtarget', 'Ġyear', 'Ġfor', 'Ġclimate', 'Ġcommitment', '?']\n",
      " \tquestion_offsets: [0, 5, 8, 12, 19, 24, 28, 36, 46]\n",
      " \tquestion_start_of_word: [1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      " \tanswers: []\n",
      " \tdocument_offsets: [0, 4, 7, 10, 20, 23, 31, 38, 46, 49, 54, 59, 62, 69, 79, 89, 92, 104, 110, 113, 115, 121, 130, 133, 139, 141, 146, 149, 156, 169, 172, 178, 180, 188, 190, 192, 199, 204, 206, 209, 215, 216, 217, 219, 226, 228, 232, 235, 237, 238, 238, 245, 248, 257, 268, 272, 282, 285, 287, 289, 298, 308, 314, 318, 324, 329, 334, 336, 344, 351, 353, 354, 356, 362, 367, 376, 382, 383, 385, 391, 393, 399, 401, 407, 411, 417, 420, 426, 428, 439, 443, 450, 454, 458, 464, 466, 470, 478, 484, 492, 503, 513, 518, 523, 526, 533, 534, 540, 543, 546, 551, 555, 558, 559, 560, 561, 563, 566, 572, 579, 588, 590, 600, 603, 611, 619, 621, 625, 632, 636, 639, 647, 650, 657, 659, 662, 666, 671, 680, 689, 696, 707]\n",
      "Features: \n",
      " \tinput_ids: [0, 2264, 16, 5, 1002, 76, 13, 2147, 2720, 116, 2, 2, 627, 2242, 354, 1288, 15, 2147, 464, 9559, 11, 570, 5026, 7, 1888, 3612, 5035, 9, 11832, 20038, 7, 10, 672, 3833, 7, 3000, 10, 1430, 11, 720, 3971, 7, 874, 132, 4176, 50, 6, 357, 202, 6, 7, 874, 112, 4, 245, 4176, 4, 33760, 3194, 17, 27, 29, 1002, 9, 4881, 11832, 1123, 5035, 30, 1814, 207, 9550, 1786, 31, 5, 1453, 76, 1466, 195, 153, 5657, 1029, 176, 242, 454, 8835, 1171, 7401, 112, 6, 7401, 132, 210, 12, 716, 8, 1667, 9, 7401, 155, 5035, 13, 1007, 8, 935, 1504, 4, 5, 2866, 716, 3247, 3893, 29548, 42, 724, 25, 2866, 12, 805, 11, 16874, 24761, 336, 6, 939, 4, 242, 4, 24, 6616, 720, 2820, 4, 309, 7, 595, 1884, 6, 5, 1002, 64, 28, 4824, 4, 959, 6, 114, 5, 7961, 5364, 3441, 723, 2706, 7267, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " \tanswer_type_ids: [0]\n",
      " \tpassage_start_t: 0\n",
      " \tstart_of_word: [0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tlabels: [[ 0  0]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]]\n",
      " \tid: [0, 0, 0]\n",
      " \tseq_2_start_t: 12\n",
      "_____________________________________________________\n",
      "10/14/2020 23:07:40 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/14/2020 23:07:40 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 1-1-0\n",
      "Clear Text: \n",
      " \tpassage_text: regulatory framework and market structure — electricity supply in colombia is concentrated in one main system, the sin, which encompasses one-third of colombia's territory, providing electricity to 97% of the country's population. the sin's installed capacity, primarily hydroelectric 68% and thermal 31%, totaled 17,392 mw as of december 31, 2018. the marked seasonal variations in colombia's hydrology result in price volatility in the short-term market. in 2018, 84% of total energy demand was supplied by hydroelectric plants.\n",
      " \tquestion_text: What is the total installed capacity from lignite (brown coal)?\n",
      " \tpassage_id: 0\n",
      " \tanswers: []\n",
      "Tokenized: \n",
      " \tpassage_start_t: 0\n",
      " \tpassage_tokens: ['reg', 'ulatory', 'Ġframework', 'Ġand', 'Ġmarket', 'Ġstructure', 'ĠâĢĶ', 'Ġelectricity', 'Ġsupply', 'Ġin', 'Ġcol', 'omb', 'ia', 'Ġis', 'Ġconcentrated', 'Ġin', 'Ġone', 'Ġmain', 'Ġsystem', ',', 'Ġthe', 'Ġsin', ',', 'Ġwhich', 'Ġencompasses', 'Ġone', '-', 'third', 'Ġof', 'Ġcol', 'omb', 'ia', \"'s\", 'Ġterritory', ',', 'Ġproviding', 'Ġelectricity', 'Ġto', 'Ġ97', '%', 'Ġof', 'Ġthe', 'Ġcountry', \"'s\", 'Ġpopulation', '.', 'Ġthe', 'Ġsin', \"'s\", 'Ġinstalled', 'Ġcapacity', ',', 'Ġprimarily', 'Ġhydro', 'electric', 'Ġ68', '%', 'Ġand', 'Ġthermal', 'Ġ31', '%,', 'Ġtotaled', 'Ġ17', ',', '392', 'Ġm', 'w', 'Ġas', 'Ġof', 'Ġde', 'cember', 'Ġ31', ',', 'Ġ2018', '.', 'Ġthe', 'Ġmarked', 'Ġseasonal', 'Ġvariations', 'Ġin', 'Ġcol', 'omb', 'ia', \"'s\", 'Ġhyd', 'rology', 'Ġresult', 'Ġin', 'Ġprice', 'Ġvolatility', 'Ġin', 'Ġthe', 'Ġshort', '-', 'term', 'Ġmarket', '.', 'Ġin', 'Ġ2018', ',', 'Ġ84', '%', 'Ġof', 'Ġtotal', 'Ġenergy', 'Ġdemand', 'Ġwas', 'Ġsupplied', 'Ġby', 'Ġhydro', 'electric', 'Ġplants', '.']\n",
      " \tpassage_offsets: [0, 3, 11, 21, 25, 32, 42, 44, 56, 63, 66, 69, 72, 75, 78, 91, 94, 98, 103, 109, 111, 115, 118, 120, 126, 138, 141, 142, 148, 151, 154, 157, 159, 162, 171, 173, 183, 195, 198, 200, 202, 205, 209, 216, 219, 229, 231, 235, 238, 241, 251, 259, 261, 271, 276, 285, 287, 289, 293, 301, 303, 306, 314, 316, 317, 321, 322, 324, 327, 330, 332, 339, 341, 343, 347, 349, 353, 360, 369, 380, 383, 386, 389, 391, 394, 397, 404, 411, 414, 420, 431, 434, 438, 443, 444, 449, 455, 457, 460, 464, 466, 468, 470, 473, 479, 486, 493, 497, 506, 509, 514, 523, 529]\n",
      " \tpassage_start_of_word: [1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      " \tquestion_tokens: ['What', 'Ġis', 'Ġthe', 'Ġtotal', 'Ġinstalled', 'Ġcapacity', 'Ġfrom', 'Ġl', 'ign', 'ite', 'Ġ(', 'brown', 'Ġcoal', ')?']\n",
      " \tquestion_offsets: [0, 5, 8, 12, 18, 28, 37, 42, 43, 46, 50, 51, 57, 61]\n",
      " \tquestion_start_of_word: [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0]\n",
      " \tanswers: []\n",
      " \tdocument_offsets: [0, 3, 11, 21, 25, 32, 42, 44, 56, 63, 66, 69, 72, 75, 78, 91, 94, 98, 103, 109, 111, 115, 118, 120, 126, 138, 141, 142, 148, 151, 154, 157, 159, 162, 171, 173, 183, 195, 198, 200, 202, 205, 209, 216, 219, 229, 231, 235, 238, 241, 251, 259, 261, 271, 276, 285, 287, 289, 293, 301, 303, 306, 314, 316, 317, 321, 322, 324, 327, 330, 332, 339, 341, 343, 347, 349, 353, 360, 369, 380, 383, 386, 389, 391, 394, 397, 404, 411, 414, 420, 431, 434, 438, 443, 444, 449, 455, 457, 460, 464, 466, 468, 470, 473, 479, 486, 493, 497, 506, 509, 514, 523, 529]\n",
      "Features: \n",
      " \tinput_ids: [0, 2264, 16, 5, 746, 5923, 2148, 31, 784, 4932, 1459, 36, 31876, 4051, 26610, 2, 2, 4950, 25911, 7208, 8, 210, 3184, 93, 4382, 1787, 11, 11311, 5223, 493, 16, 15450, 11, 65, 1049, 467, 6, 5, 10272, 6, 61, 27747, 65, 12, 12347, 9, 11311, 5223, 493, 18, 4284, 6, 1976, 4382, 7, 8783, 207, 9, 5, 247, 18, 1956, 4, 5, 10272, 18, 5923, 2148, 6, 4212, 13575, 18986, 5595, 207, 8, 17210, 1105, 4234, 15137, 601, 6, 36350, 475, 605, 25, 9, 263, 47153, 1105, 6, 199, 4, 5, 4760, 10175, 18746, 11, 11311, 5223, 493, 18, 13075, 27895, 898, 11, 425, 4975, 11, 5, 765, 12, 1279, 210, 4, 11, 199, 6, 7994, 207, 9, 746, 1007, 1077, 21, 12359, 30, 13575, 18986, 3451, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " \tanswer_type_ids: [0]\n",
      " \tpassage_start_t: 0\n",
      " \tstart_of_word: [0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tlabels: [[ 0  0]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]]\n",
      " \tid: [1, 1, 0]\n",
      " \tseq_2_start_t: 17\n",
      "_____________________________________________________\n",
      "10/14/2020 23:07:40 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 0-1-0\n",
      "Clear Text: \n",
      " \tpassage_text: hydro assets in panama have ppas with distribution companies up to december 2030 for a total contracted capacity of 350 mw. thermal\n",
      " \tquestion_text: What is the total installed capacity from lignite (brown coal)?\n",
      " \tpassage_id: 0\n",
      " \tanswers: []\n",
      "Tokenized: \n",
      " \tpassage_start_t: 0\n",
      " \tpassage_tokens: ['hyd', 'ro', 'Ġassets', 'Ġin', 'Ġpan', 'ama', 'Ġhave', 'Ġpp', 'as', 'Ġwith', 'Ġdistribution', 'Ġcompanies', 'Ġup', 'Ġto', 'Ġde', 'cember', 'Ġ2030', 'Ġfor', 'Ġa', 'Ġtotal', 'Ġcontracted', 'Ġcapacity', 'Ġof', 'Ġ350', 'Ġm', 'w', '.', 'Ġthermal']\n",
      " \tpassage_offsets: [0, 3, 6, 13, 16, 19, 23, 28, 30, 33, 38, 51, 61, 64, 67, 69, 76, 81, 85, 87, 93, 104, 113, 116, 120, 121, 122, 124]\n",
      " \tpassage_start_of_word: [1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1]\n",
      " \tquestion_tokens: ['What', 'Ġis', 'Ġthe', 'Ġtotal', 'Ġinstalled', 'Ġcapacity', 'Ġfrom', 'Ġl', 'ign', 'ite', 'Ġ(', 'brown', 'Ġcoal', ')?']\n",
      " \tquestion_offsets: [0, 5, 8, 12, 18, 28, 37, 42, 43, 46, 50, 51, 57, 61]\n",
      " \tquestion_start_of_word: [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0]\n",
      " \tanswers: []\n",
      " \tdocument_offsets: [0, 3, 6, 13, 16, 19, 23, 28, 30, 33, 38, 51, 61, 64, 67, 69, 76, 81, 85, 87, 93, 104, 113, 116, 120, 121, 122, 124]\n",
      "Features: \n",
      " \tinput_ids: [0, 2264, 16, 5, 746, 5923, 2148, 31, 784, 4932, 1459, 36, 31876, 4051, 26610, 2, 2, 30420, 1001, 1781, 11, 5730, 2583, 33, 33906, 281, 19, 3854, 451, 62, 7, 263, 47153, 12060, 13, 10, 746, 13196, 2148, 9, 10088, 475, 605, 4, 17210, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " \tanswer_type_ids: [0]\n",
      " \tpassage_start_t: 0\n",
      " \tstart_of_word: [0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tlabels: [[ 0  0]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]]\n",
      " \tid: [0, 1, 0]\n",
      " \tseq_2_start_t: 17\n",
      "_____________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from farm.infer import QAInferencer\n",
    "\n",
    "model = QAInferencer.load(file_config.saved_models_dir, batch_size=40, gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's make prediction on a pair of paragraph and question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"the paris agreement on climate change drafted in 2015 aims to reduce worldwide emissions of greenhouse \n",
    "gases to a level intended to limit a rise in global temperatures to below 2 degrees or, better still,\n",
    "to below 1.5 degrees. verbund’s target of reducing greenhouse gas emissions by 90% measured beginning from \n",
    "the basis year 2011 5 million tonnes co2e until 2021 includes scope 1, scope 2 market- based and parts of scope 3 emissions \n",
    "for energy and air travel. the science based targets initiative validated this goal as science-based in october 2016, \n",
    "i.e. it meets global standards. according to current planning, the target can be achieved. \n",
    "however, if the grid operator requires higher generation volumes \n",
    "\"\"\"\n",
    "question = \"What is the target year for climate commitment?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00, 21.48 Batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [{'answers': [{'answer': '2021',\n",
      "                               'context': 'the basis year 2011 5 million '\n",
      "                                          'tonnes co2e until 2021 includes '\n",
      "                                          'scope 1, scope 2 market- based and '\n",
      "                                          'par',\n",
      "                               'document_id': '0-0',\n",
      "                               'offset_answer_end': 366,\n",
      "                               'offset_answer_start': 362,\n",
      "                               'offset_context_end': 414,\n",
      "                               'offset_context_start': 314,\n",
      "                               'probability': None,\n",
      "                               'score': 8.748405456542969},\n",
      "                              {'answer': 'no_answer',\n",
      "                               'context': '',\n",
      "                               'document_id': '0-0',\n",
      "                               'offset_answer_end': 0,\n",
      "                               'offset_answer_start': 0,\n",
      "                               'offset_context_end': 0,\n",
      "                               'offset_context_start': 0,\n",
      "                               'probability': None,\n",
      "                               'score': -2.1953351497650146}],\n",
      "                  'ground_truth': [],\n",
      "                  'id': '0-0',\n",
      "                  'no_ans_gap': 10.943740606307983,\n",
      "                  'question': 'What is the target year for climate '\n",
      "                              'commitment?'}],\n",
      " 'task': 'qa'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "QA_input = [{\"qas\": [question], \"context\": context}]\n",
    "\n",
    "result = model.inference_from_dicts(dicts=QA_input)[0]\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the prediction result show? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 8.748405456542969,\n",
       " 'probability': None,\n",
       " 'answer': '2021',\n",
       " 'offset_answer_start': 362,\n",
       " 'offset_answer_end': 366,\n",
       " 'context': 'the basis year 2011 5 million tonnes co2e until 2021 includes scope 1, scope 2 market- based and par',\n",
       " 'offset_context_start': 314,\n",
       " 'offset_context_end': 414,\n",
       " 'document_id': '0-0'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the best answer. Generally it can be span-based or it can be no-answer, which ever is higher\n",
    "# Here the top answer is the span '2021'\n",
    "result[\"predictions\"][0][\"answers\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': -2.1953351497650146,\n",
       " 'probability': None,\n",
       " 'answer': 'no_answer',\n",
       " 'offset_answer_start': 0,\n",
       " 'offset_answer_end': 0,\n",
       " 'context': '',\n",
       " 'offset_context_start': 0,\n",
       " 'offset_context_end': 0,\n",
       " 'document_id': '0-0'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Non-answerable score: The model is pretty confident that the answer to the question can be in the context.\n",
    "result[\"predictions\"][0][\"answers\"][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also make the prediction on a squad-format file as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.46s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.60 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.57 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.75 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.77 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.08 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.93 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.94 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.41 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.98 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.04 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.29 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.38 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.03 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.60 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.31 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.89 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.15 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.01 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.83 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.90 Batches/s]\n",
      "10/14/2020 23:07:51 - ERROR - farm.modeling.predictions -   Both start and end offsets should be 0: \n",
      "558, 558 with a no_answer. \n",
      "10/14/2020 23:07:51 - ERROR - farm.modeling.predictions -   Both start and end offsets should be 0: \n",
      "558, 558 with a no_answer. \n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.26 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.59 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.81 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.91 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.87 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.60 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.52 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.80 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.92 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  8.14 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.89 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.85 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  8.16 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.91 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.86 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.92 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.57 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.89 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.93 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  8.12 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  8.14 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.07 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.92 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.88 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.84 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  8.20 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.04 Batches/s]\n",
      "10/14/2020 23:07:56 - INFO - farm.data_handler.utils -   There were missing predicitons for question ids: {0, 1, 2, 2058, 10, 11, 12, 18, 19, 26, 35, 2086, 2090, 42, 43, 2095, 49, 2099, 2100, 53, 55, 60, 68, 2118, 2120, 2121, 73, 76, 2129, 82, 2131, 2133, 87, 88, 2138, 2140, 2142, 2153, 2157, 113, 116, 124, 2174, 2175, 137, 2190, 2191, 2192, 145, 149, 150, 2199, 2201, 2203, 158, 2208, 2210, 2212, 172, 174, 175, 2224, 176, 2228, 2233, 2234, 190, 193, 195, 200, 2251, 2252, 2253, 211, 212, 2261, 2263, 215, 2265, 2270, 2272, 2274, 2286, 2291, 243, 2299, 2303, 2305, 257, 258, 265, 2317, 2318, 269, 273, 2325, 2326, 279, 278, 2329, 2331, 2334, 2335, 288, 2346, 2347, 2352, 2353, 306, 304, 305, 2357, 2358, 2363, 315, 317, 2366, 338, 2390, 343, 348, 349, 353, 354, 362, 363, 364, 366, 375, 382, 385, 386, 397, 398, 401, 405, 406, 409, 416, 418, 422, 423, 424, 425, 431, 438, 440, 446, 447, 448, 450, 454, 455, 456, 460, 469, 475, 476, 483, 492, 493, 494, 495, 499, 506, 511, 513, 519, 523, 524, 525, 526, 528, 529, 530, 565, 566, 570, 571, 588, 592, 597, 617, 618, 619, 620, 621, 635, 645, 646, 676, 703, 707, 739, 765, 780, 852, 855, 859, 865, 867, 869, 872, 874, 881, 884, 889, 892, 897, 900, 901, 905, 906, 909, 911, 920, 922, 924, 925, 929, 930, 932, 941, 944, 945, 949, 950, 952, 961, 966, 968, 969, 972, 980, 982, 988, 991, 998, 1000, 1001, 1005, 1008, 1009, 1010, 1011, 1014, 1021, 1023, 1029, 1032, 1037, 1040, 1043, 1044, 1045, 1052, 1054, 1060, 1066, 1071, 1074, 1075, 1085, 1088, 1092, 1102, 1106, 1111, 1113, 1118, 1124, 1126, 1143, 1145, 1147, 1152, 1157, 1159, 1168, 1170, 1171, 1184, 1190, 1191, 1192, 1199, 1200, 1202, 1208, 1213, 1214, 1215, 1217, 1221, 1229, 1235, 1236, 1239, 1240, 1241, 1243, 1245, 1248, 1249, 1250, 1258, 1265, 1266, 1268, 1270, 1272, 1275, 1281, 1287, 1288, 1291, 1292, 1296, 1299, 1305, 1306, 1308, 1309, 1312, 1314, 1317, 1319, 1320, 1328, 1330, 1331, 1336, 1340, 1342, 1345, 1349, 1353, 1357, 1359, 1367, 1379, 1381, 1385, 1403, 1407, 1410, 1412, 1419, 1430, 1432, 1436, 1448, 1453, 1457, 1461, 1472, 1475, 1483, 1487, 1490, 1493, 1495, 1496, 1497, 1498, 1506, 1509, 1512, 1515, 1518, 1523, 1525, 1527, 1528, 1530, 1546, 1547, 1551, 1556, 1559, 1562, 1566, 1568, 1576, 1577, 1579, 1580, 1593, 1596, 1599, 1600, 1607, 1612, 1614, 1615, 1623, 1630, 1633, 1635, 1641, 1645, 1650, 1654, 1660, 1662, 1663, 1665, 1670, 1672, 1673, 1682, 1689, 1695, 1696, 1700, 1701, 1707, 1711, 1716, 1720, 1726, 1731, 1732, 1733, 1738, 1740, 1741, 1754, 1757, 1758, 1764, 1768, 1772, 1779, 1781, 1783, 1786, 1788, 1789, 1801, 1803, 1807, 1810, 1814, 1820, 1821, 1822, 1825, 1826, 1834, 1839, 1842, 1844, 1846, 1847, 1854, 1857, 1860, 1868, 1869, 1876, 1881, 1884, 1894, 1895, 1903, 1911, 1917, 1918, 1931, 1932, 1935, 1937, 1938, 1944, 1951, 1952, 1972, 1973, 1974, 1977, 1979, 1980, 1985, 1988, 1989, 1999, 2000, 2001, 2003, 2004, 2017, 2046}\n",
      "10/14/2020 23:07:56 - INFO - farm.data_handler.utils -   Written Squad predictions to: /model_pipeline/model_pipeline/data/predictions.json\n"
     ]
    }
   ],
   "source": [
    "from farm.data_handler.utils import write_squad_predictions\n",
    "\n",
    "results = model.inference_from_file(file=file_config.dev_filename, return_json=False)\n",
    "result_squad = [x.to_squad_eval() for x in results]\n",
    "\n",
    "write_squad_predictions(\n",
    "    predictions=result_squad,\n",
    "    predictions_filename=file_config.dev_filename,\n",
    "    out_filename=os.path.join(file_config.data_dir, \"predictions.json\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is written in the `out_filename`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Tip_: Many of the result objects belong to farm classes. If you want to see the attributes of its class in jupyter notebook, type the \"objectname.\" and then press tap. For example, try it in the below cell by pressing tap and see the attribiues of the class. This is an usefull jupyter notebook trick. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the cursor after dot and press Tab.\n",
    "result[0]."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
